# 🔴 模型幻觉问题分析报告

## 问题描述

**用户查询**：湖北大悟农村商业银行股份有限公司兴发支行

**模型返回**：湖北大悟农村商业银行股份有限公司兴发支行的联行号是**313164400027**

**正确答案**：湖北大悟农村商业银行股份有限公司兴发支行的联行号是**402535510938**

**问题**：模型生成了错误的联行号，与训练数据不一致

## 根本原因分析

### 1. 模型幻觉（Hallucination）

**什么是模型幻觉**：
- 模型生成了看起来合理但实际错误的内容
- 联行号 `313164400027` 是12位数字，格式正确
- 但这个号码**不在训练数据中**，是模型"编造"的

**为什么会产生幻觉**：
1. **训练数据量不足**：只有400个QA对（320训练+40验证+40测试）
2. **模型规模太小**：Qwen2.5-1.5B参数量有限，记忆能力不足
3. **训练轮数不够**：只训练了3个epoch，模型还没有充分学习
4. **LoRA秩太小**：r=8，可训练参数只有0.14%，表达能力有限

### 2. 训练数据验证

**正确的训练数据**（已确认）：
```sql
湖北大悟农村商业银行股份有限公司兴发支行的联行号是什么？ -> 402535510938
大悟农商行的联行号 -> 湖北大悟农村商业银行股份有限公司兴发支行的联行号是402535510938
农商行兴发支行的联行号 -> 湖北大悟农村商业银行股份有限公司兴发支行的联行号是402535510938
```

**数据库中的记录**：
```
bank_name: 湖北大悟农村商业银行股份有限公司兴发支行
bank_code: 402535510938
clearing_code: 402521000000
```

**错误的联行号**：
- `313164400027` 不在数据库中
- 不在训练数据中
- 是模型"幻想"出来的

### 3. 日志分析

**模型实际生成的内容**：
```
湖北大悟农村商业银行股份有限公司兴发支行的联行号是313164400027
兴发支行|湖北大悟农村商业银行股份有限公司|兴发支行的联行号是313164400027
兴发农商行|湖北大悟农村商业银行股份有限公司|兴发支行的联行号是313164400027
```

**观察**：
- 模型生成了多个变体，但都是错误的联行号
- 说明模型没有记住正确的映射关系
- 而是在"猜测"一个看起来合理的数字

## 解决方案

### 方案1：增加训练数据量（推荐）⭐⭐⭐⭐⭐

**当前数据量**：400个QA对
**建议数据量**：至少2000-5000个QA对

**如何增加**：
1. 为每个银行生成更多变体问题
2. 增加同义词、简称、全称的组合
3. 添加更多上下文信息

**示例**：
```python
# 当前：每个银行约5-7个问题
# 建议：每个银行至少10-15个问题

原始数据：湖北大悟农村商业银行股份有限公司兴发支行 -> 402535510938

生成更多变体：
1. "湖北大悟农村商业银行股份有限公司兴发支行的联行号"
2. "大悟农商行兴发支行联行号"
3. "湖北大悟兴发支行的银行代码"
4. "402535510938是哪个银行"
5. "查询湖北大悟农商行兴发支行"
6. "兴发支行在大悟的联行号"
7. "大悟县兴发支行联行号查询"
8. "湖北省大悟农商行兴发支行代码"
9. "农商行兴发支行（大悟）联行号"
10. "大悟农村商业银行兴发支行号码"
... 更多变体
```

### 方案2：增加训练轮数（简单有效）⭐⭐⭐⭐

**当前配置**：3个epoch
**建议配置**：10-20个epoch

**修改方法**：
在训练任务创建时，将epochs从3改为10或更多

**注意**：
- 需要监控验证集loss，避免过拟合
- 如果验证loss开始上升，说明过拟合了

### 方案3：增大LoRA秩（提升表达能力）⭐⭐⭐⭐

**当前配置**：
- lora_r = 8
- lora_alpha = 16
- 可训练参数：0.14%

**建议配置**：
- lora_r = 32 或 64
- lora_alpha = 64 或 128
- 可训练参数：0.5-1%

**权衡**：
- 更大的秩 = 更强的表达能力 = 更好的记忆
- 但也 = 更多的显存占用 = 更慢的训练速度

### 方案4：使用更大的基础模型（最有效但最慢）⭐⭐⭐⭐⭐

**当前模型**：Qwen2.5-1.5B（1.5B参数）
**建议模型**：Qwen2.5-3B 或 Qwen2.5-7B

**优势**：
- 更强的理解和记忆能力
- 更少的幻觉
- 更准确的答案

**劣势**：
- 需要更多显存（M1 16GB可能不够用于7B）
- 训练时间更长
- 推理速度更慢

### 方案5：降低生成温度（减少随机性）⭐⭐⭐

**当前配置**：
```python
temperature = 0.7
top_p = 0.9
```

**建议配置**：
```python
temperature = 0.1  # 更确定性
top_p = 0.8        # 更保守
```

**修改位置**：`mvp/app/services/query_service.py` 的 `generate_answer()` 方法

### 方案6：添加检索增强生成（RAG）⭐⭐⭐⭐⭐

**原理**：
1. 先在数据库中检索相关银行
2. 将检索结果作为上下文提供给模型
3. 模型基于上下文生成答案

**优势**：
- 大幅减少幻觉
- 答案更准确
- 可以处理训练数据中没有的银行

**实现**：
```python
def query_with_rag(question: str):
    # 1. 从问题中提取银行名称关键词
    keywords = extract_keywords(question)
    
    # 2. 在数据库中检索相关银行
    candidates = db.query(BankCode).filter(
        BankCode.bank_name.contains(keywords)
    ).limit(5).all()
    
    # 3. 构建上下文
    context = "\n".join([
        f"{bank.bank_name}: {bank.bank_code}"
        for bank in candidates
    ])
    
    # 4. 将上下文加入提示
    prompt = f"""参考信息：
{context}

Question: {question}
Answer:"""
    
    # 5. 生成答案
    answer = model.generate(prompt)
    return answer
```

## 推荐的综合方案

### 短期方案（立即可做）

1. **降低生成温度**：temperature=0.1, top_p=0.8
2. **增加训练轮数**：从3个epoch增加到10个epoch
3. **增大LoRA秩**：r=32, alpha=64

### 中期方案（需要准备数据）

4. **增加训练数据量**：
   - 为每个银行生成10-15个问题变体
   - 目标：2000-5000个QA对
   - 使用QA生成器自动生成

### 长期方案（最佳效果）

5. **实现RAG系统**：
   - 先检索数据库
   - 再让模型基于检索结果生成答案
   - 这是最可靠的方案

6. **使用更大模型**：
   - 如果硬件允许，升级到Qwen2.5-3B

## 立即可执行的修复

让我先帮你实现**短期方案**：

### 1. 降低生成温度（减少随机性）

修改 `mvp/app/services/query_service.py`：
```python
def generate_answer(
    self,
    question: str,
    max_new_tokens: int = 256,
    temperature: float = 0.1,  # 从0.7降低到0.1
    top_p: float = 0.8          # 从0.9降低到0.8
) -> str:
```

### 2. 重新训练（增加轮数和LoRA秩）

在前端创建新的训练任务时：
- Epochs: 10（而不是3）
- LoRA Rank (r): 32（而不是8）
- LoRA Alpha: 64（而不是16）

## 验证方法

重新训练后，测试以下问题：
1. "湖北大悟农村商业银行股份有限公司兴发支行"
2. "工商银行北京市知春路支行联行号"
3. "中国银行的银行代码"

**期望结果**：
- 联行号与训练数据一致
- 置信度 > 0.5
- 能正确提取到匹配记录

## 技术原理

### 为什么小模型容易产生幻觉

1. **参数量限制**：
   - 1.5B参数需要记住2889个银行的映射
   - 平均每个银行只有约50万个参数
   - 不足以准确记忆

2. **训练数据不足**：
   - 400个样本 / 2889个银行 = 每个银行平均0.14个样本
   - 大部分银行只在训练数据中出现1次
   - 模型没有足够的重复来记忆

3. **LoRA秩太小**：
   - r=8意味着只有0.14%的参数可训练
   - 表达能力严重受限
   - 无法学习复杂的映射关系

### RAG为什么有效

1. **不依赖记忆**：
   - 直接从数据库检索准确信息
   - 模型只需要理解和重组信息
   - 不需要记住所有映射关系

2. **零幻觉**：
   - 答案基于真实数据
   - 不会编造不存在的联行号
   - 可以处理训练数据外的查询

3. **可扩展**：
   - 添加新银行不需要重新训练
   - 只需更新数据库
   - 立即可用

## 下一步行动

请选择：

**A. 立即修复（5分钟）**
- 降低生成温度
- 重启服务测试

**B. 重新训练（30分钟）**
- 使用更好的参数（epochs=10, r=32）
- 等待训练完成
- 测试新模型

**C. 实现RAG（1小时）**
- 添加数据库检索逻辑
- 修改提示格式
- 彻底解决幻觉问题

**D. 生成更多数据（2小时）**
- 使用QA生成器生成2000+样本
- 重新训练
- 获得最佳效果

我建议先做**A+B**，然后考虑**C**（RAG是最可靠的长期方案）。

---

**报告时间**：2026-01-21 16:30
**问题级别**：🔴 严重（影响答案准确性）
**建议优先级**：⭐⭐⭐⭐⭐ 高优先级
