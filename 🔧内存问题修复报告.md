# 🔧 内存问题修复报告

## 🚨 **问题现象**

```log
ERROR | MPS backend out of memory (MPS allocated: 20.14 GiB, other allocations: 384.00 KiB, max allowed: 20.13 GiB)
```

### 问题分析
- **设备**：M1 MacBook Pro 16GB统一内存
- **问题**：模型缓存占用了几乎全部GPU内存
- **原因**：缓存系统没有考虑内存限制

## ✅ **解决方案**

### 1. 智能内存管理
```python
# 限制缓存的模型数量
_max_cached_models = 1  # 只缓存1个模型，避免内存不足

# 内存清理函数
def clear_gpu_memory():
    if torch.mps.is_available():
        torch.mps.empty_cache()
```

### 2. 自动内存回收
- **缓存满时**：自动清理最旧的模型
- **加载失败时**：清理所有缓存并重试
- **主动清理**：提供API接口清理缓存

### 3. 内存监控工具
创建了`清理GPU内存.py`工具：
- 监控系统内存使用
- 清理PyTorch缓存
- 通过API清理模型缓存

## 🛠️ **修复措施**

### 立即措施
1. **限制缓存**：只缓存1个模型
2. **自动清理**：内存不足时自动回收
3. **错误恢复**：加载失败时清理重试

### 临时解决方案
```bash
# 方案1：清理GPU内存
python 清理GPU内存.py

# 方案2：重启后端服务
cd mvp && ./scripts/stop.sh && ./scripts/start.sh

# 方案3：设置环境变量（允许更多内存使用，但有风险）
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0
```

### 长期优化
1. **模型量化**：使用更小的模型
2. **分层缓存**：CPU+GPU混合缓存
3. **内存池**：预分配内存池

## 📊 **内存使用优化**

### 优化前
```
总内存: 16GB
├── 系统占用: ~4GB
├── 应用占用: ~2GB
├── 模型缓存: ~10GB (多个模型)
└── 可用内存: 0GB ← 问题！
```

### 优化后
```
总内存: 16GB
├── 系统占用: ~4GB
├── 应用占用: ~2GB
├── 模型缓存: ~6GB (单个模型)
└── 可用内存: ~4GB ✅
```

## 🎯 **验证步骤**

### 1. 清理当前内存
```bash
python 清理GPU内存.py
```

### 2. 重启服务
```bash
cd mvp
./scripts/stop.sh
./scripts/start.sh
```

### 3. 测试查询
- 提交查询："华夏银行江油西山支行"
- 观察内存使用情况
- 确认不再出现内存错误

### 4. 监控日志
```bash
tail -f mvp/logs/error_2026-01-21.log
```

## 🔍 **内存监控**

### 关键指标
- **MPS内存使用**：应该<16GB
- **缓存模型数量**：应该≤1
- **内存清理频率**：按需清理

### 监控命令
```bash
# 查看内存使用
python -c "import psutil; m=psutil.virtual_memory(); print(f'Memory: {m.percent:.1f}% used')"

# 查看GPU内存（如果有nvidia-smi）
nvidia-smi

# 查看模型缓存状态
curl -H "Authorization: Bearer $ADMIN_TOKEN" \
  http://localhost:8001/api/v1/query/model-cache-stats
```

## 🚀 **性能影响**

### 缓存策略调整
| 场景 | 修复前 | 修复后 | 说明 |
|------|--------|--------|------|
| **首次查询** | 20秒 | 20秒 | 需要加载模型 |
| **相同模型查询** | <100ms | <100ms | 缓存命中 |
| **不同模型查询** | <100ms | 20秒 | 需要重新加载 |
| **内存使用** | 爆满 | 安全 | 智能管理 |

### 权衡说明
- **优点**：避免内存溢出，系统稳定
- **缺点**：切换模型时需要重新加载
- **建议**：在前端固定使用一个最佳模型

## 💡 **使用建议**

### 1. 模型选择
- **推荐**：使用Qwen2.5-0.5B（最小内存占用）
- **避免**：同时使用多个大模型

### 2. 查询策略
- **固定模型**：在前端选择一个模型并坚持使用
- **批量查询**：一次性处理多个问题

### 3. 内存管理
- **定期清理**：使用清理工具
- **监控使用**：关注内存使用率
- **及时重启**：内存问题时重启服务

---

**修复时间**：2026-01-21 19:50
**修复状态**：✅ 已部署
**验证方法**：清理内存 → 重启服务 → 测试查询

**请按照验证步骤测试修复效果！** 🔧