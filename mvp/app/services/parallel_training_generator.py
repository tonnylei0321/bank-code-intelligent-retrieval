"""
å¹¶è¡Œè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨

ä½¿ç”¨å¤šçº¿ç¨‹å’Œå¤šä¸ªLLM APIå¹¶è¡Œç”Ÿæˆå¤§è§„æ¨¡è®­ç»ƒæ•°æ®
æ”¯æŒ15ä¸‡æ¡æ ·æœ¬æ•°æ®ï¼Œæ¯æ¡ç”Ÿæˆ7ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œæ€»è®¡105ä¸‡æ¡è®­ç»ƒæ•°æ®

ç‰¹æ€§ï¼š
1. å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
2. å¤šä¸ªLLM APIè´Ÿè½½å‡è¡¡
3. æ•°æ®åº“æ‰¹é‡å†™å…¥ä¼˜åŒ–
4. è¿›åº¦ç›‘æ§å’Œé”™è¯¯æ¢å¤
5. å†…å­˜ä¼˜åŒ–å’Œèµ„æºç®¡ç†

ä½œè€…ï¼šAI Assistant
æ—¥æœŸï¼š2026-01-30
"""

import asyncio
import aiohttp
import json
import time
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from queue import Queue
import logging
from datetime import datetime
import random

from sqlalchemy.orm import Session
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

from app.models.bank_code import BankCode
from app.models.qa_pair import QAPair
from app.models.dataset import Dataset
from app.core.database import get_db, engine

logger = logging.getLogger(__name__)


@dataclass
class LLMConfig:
    """LLMé…ç½®"""
    name: str
    base_url: str
    api_key: str
    model_name: str
    max_requests_per_minute: int = 60


class ParallelTrainingGenerator:
    """
    å¹¶è¡Œè®­ç»ƒæ•°æ®ç”Ÿæˆå™¨
    
    ä½¿ç”¨å¤šä¸ªLLM APIå¹¶è¡Œç”Ÿæˆè®­ç»ƒæ•°æ®ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®å¤„ç†
    """
    
    def __init__(self, dataset_id: int, progress_callback: Optional[callable] = None):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            dataset_id: æ•°æ®é›†ID
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶statså­—å…¸å‚æ•°
        """
        self.dataset_id = dataset_id
        self.progress_callback = progress_callback
        
        # é…ç½®å¤šä¸ªLLM
        self.llm_configs = [
            LLMConfig(
                name="é˜¿é‡Œé€šä¹‰åƒé—®",
                base_url="https://dashscope.aliyuncs.com/api/v1/services/aigc",
                api_key="sk-03f639acddb8425abd3c1b9722ec1014",
                model_name="qwen-turbo",
                max_requests_per_minute=100
            ),
            LLMConfig(
                name="DeepSeek",
                base_url="https://api.deepseek.com",
                api_key="sk-9b923042a7714c9cb68ff338ab68d36d",
                model_name="deepseek-chat",
                max_requests_per_minute=100
            )
        ]
        
        # çº¿ç¨‹æ± é…ç½® - ä¼˜åŒ–å¤§æ•°æ®é›†
        if hasattr(self, '_bank_count') and self._bank_count > 50000:
            self.max_workers = 16  # å¤§æ•°æ®é›†ä½¿ç”¨æ›´å¤šçº¿ç¨‹
        else:
            self.max_workers = 8  # æ¯ä¸ªLLM 4ä¸ªçº¿ç¨‹ï¼Œ2ä¸ªLLMæ€»å…±8ä¸ªçº¿ç¨‹
        self.batch_size = 100  # æ•°æ®åº“æ‰¹é‡å†™å…¥å¤§å°
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_banks": 0,
            "processed_banks": 0,
            "generated_samples": 0,
            "failed_banks": 0,
            "start_time": None,
            "errors": []
        }
        
        # çº¿ç¨‹å®‰å…¨çš„é˜Ÿåˆ—
        self.result_queue = Queue()
        self.error_queue = Queue()
        
        # æ•°æ®åº“ä¼šè¯å·¥å‚
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
        
        logger.info(f"ParallelTrainingGenerator initialized with {len(self.llm_configs)} LLMs")
    
    def configure_llm(self, llm_name: str):
        """
        æ ¹æ®ç”¨æˆ·é€‰æ‹©é…ç½®å•ä¸ªLLM
        
        Args:
            llm_name: LLMåç§° ("qwen", "deepseek", "chatglm")
        """
        if llm_name == "qwen":
            self.llm_configs = [
                LLMConfig(
                    name="é˜¿é‡Œé€šä¹‰åƒé—®",
                    base_url="https://dashscope.aliyuncs.com/api/v1/services/aigc",
                    api_key="sk-03f639acddb8425abd3c1b9722ec1014",
                    model_name="qwen-turbo",
                    max_requests_per_minute=100
                )
            ]
        elif llm_name == "deepseek":
            self.llm_configs = [
                LLMConfig(
                    name="DeepSeek",
                    base_url="https://api.deepseek.com",
                    api_key="sk-9b923042a7714c9cb68ff338ab68d36d",
                    model_name="deepseek-chat",
                    max_requests_per_minute=100
                )
            ]
        elif llm_name == "chatglm":
            self.llm_configs = [
                LLMConfig(
                    name="æ™ºè°±ChatGLM",
                    base_url="https://open.bigmodel.cn/api/paas/v4",
                    api_key="your-chatglm-api-key",  # éœ€è¦é…ç½®å®é™…çš„APIå¯†é’¥
                    model_name="glm-4",
                    max_requests_per_minute=100
                )
            ]
        else:
            # é»˜è®¤ä½¿ç”¨å¤šLLMå¹¶è¡Œ
            pass
        
        logger.info(f"Configured LLM: {llm_name}, using {len(self.llm_configs)} LLM(s)")
    
    def set_bank_count(self, count: int):
        """è®¾ç½®é“¶è¡Œæ•°é‡ä»¥ä¼˜åŒ–é…ç½®"""
        self._bank_count = count
        if count > 50000:
            self.max_workers = 16
            logger.info(f"Large dataset detected ({count:,} banks), using {self.max_workers} workers")
    
    async def generate_samples_async(
        self, 
        bank_name: str, 
        bank_code: str, 
        bank_id: int,
        llm_config: LLMConfig,
        session: aiohttp.ClientSession,
        samples_per_bank: int = 7
    ) -> List[Dict[str, Any]]:
        """
        å¼‚æ­¥ç”Ÿæˆå•ä¸ªé“¶è¡Œçš„è®­ç»ƒæ ·æœ¬
        
        Args:
            bank_name: é“¶è¡Œåç§°
            bank_code: è”è¡Œå·
            bank_id: é“¶è¡ŒID
            llm_config: LLMé…ç½®
            session: HTTPä¼šè¯
            samples_per_bank: æ¯ä¸ªé“¶è¡Œç”Ÿæˆçš„æ ·æœ¬æ•°é‡
            
        Returns:
            è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        """
        try:
            print(f"ğŸ”§ DEBUG: generate_samples_async started for {bank_name} using {llm_config.name}")
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_prompt(bank_name, bank_code, samples_per_bank)
            print(f"ğŸ”§ DEBUG: Built prompt for {bank_name}, length: {len(prompt)}")
            
            # è°ƒç”¨LLM API
            print(f"ğŸ”§ DEBUG: Calling LLM API for {bank_name} using {llm_config.name}")
            response = await self._call_llm_api(prompt, llm_config, session)
            print(f"ğŸ”§ DEBUG: LLM API response received for {bank_name}, length: {len(response) if response else 0}")
            
            # è§£æå“åº”
            samples = self._parse_llm_response(response, bank_name, bank_code, bank_id, samples_per_bank)
            print(f"ğŸ”§ DEBUG: Parsed {len(samples)} samples for {bank_name}")
            
            logger.debug(f"Generated {len(samples)} samples for {bank_name} using {llm_config.name}")
            return samples
            
        except Exception as e:
            logger.error(f"Failed to generate samples for {bank_name} using {llm_config.name}: {e}")
            # è¿”å›è§„åˆ™ç”Ÿæˆçš„æ ·æœ¬ä½œä¸ºå¤‡ç”¨
            return self._generate_rule_based_samples(bank_name, bank_code, bank_id, samples_per_bank)
    
    def _build_prompt(self, bank_name: str, bank_code: str, samples_per_bank: int = 7) -> str:
        """æ„å»ºLLMæç¤ºè¯"""
        return f"""ä½ æ˜¯ä¸€ä¸ªé“¶è¡Œä¸šåŠ¡ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹é“¶è¡Œç”Ÿæˆ{samples_per_bank}ç§ä¸åŒçš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢æ–¹å¼ã€‚

é“¶è¡Œä¿¡æ¯ï¼š
- å®Œæ•´åç§°ï¼š{bank_name}
- è”è¡Œå·ï¼š{bank_code}

è¦æ±‚ï¼š
1. ç”Ÿæˆ{samples_per_bank}ç§ç”¨æˆ·å¯èƒ½çš„é—®æ³•
2. åŒ…æ‹¬ï¼šå®Œæ•´åç§°ã€ç®€ç§°ã€å£è¯­åŒ–è¡¨è¾¾ã€åœ°åŒº+é“¶è¡Œåã€ä¸å®Œæ•´æè¿°ç­‰
3. æ¨¡æ‹ŸçœŸå®ç”¨æˆ·çš„æŸ¥è¯¢ä¹ æƒ¯ï¼ˆç®€çŸ­ã€è‡ªç„¶ã€å£è¯­åŒ–ï¼‰
4. æ¯ç§é—®æ³•è¦è‡ªç„¶ã€ç®€æ´ï¼Œä¸è¦å¤ªé•¿

è¯·ç›´æ¥è¿”å›JSONæ ¼å¼ï¼ˆä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
    "questions": [
        "é—®æ³•1",
        "é—®æ³•2", 
        "é—®æ³•3",
        "é—®æ³•4",
        "é—®æ³•5",
        "é—®æ³•6",
        "é—®æ³•7"
    ]
}}

ç°åœ¨è¯·ä¸ºä¸Šè¿°é“¶è¡Œç”Ÿæˆ{samples_per_bank}ç§é—®æ³•ï¼š"""
    
    async def _call_llm_api(
        self, 
        prompt: str, 
        llm_config: LLMConfig, 
        session: aiohttp.ClientSession
    ) -> str:
        """
        è°ƒç”¨LLM API
        
        Args:
            prompt: æç¤ºè¯
            llm_config: LLMé…ç½®
            session: HTTPä¼šè¯
            
        Returns:
            LLMå“åº”æ–‡æœ¬
        """
        print(f"ğŸ”§ DEBUG: _call_llm_api started for {llm_config.name}")
        
        if llm_config.name == "é˜¿é‡Œé€šä¹‰åƒé—®":
            return await self._call_qwen_api(prompt, llm_config, session)
        elif llm_config.name == "DeepSeek":
            return await self._call_deepseek_api(prompt, llm_config, session)
        else:
            raise ValueError(f"Unsupported LLM: {llm_config.name}")
    
    async def _call_qwen_api(
        self, 
        prompt: str, 
        llm_config: LLMConfig, 
        session: aiohttp.ClientSession
    ) -> str:
        """è°ƒç”¨é˜¿é‡Œé€šä¹‰åƒé—®API"""
        print(f"ğŸ”§ DEBUG: _call_qwen_api started")
        
        headers = {
            "Authorization": f"Bearer {llm_config.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": llm_config.model_name,
            "input": {
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é“¶è¡Œä¸šåŠ¡åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            },
            "parameters": {
                "temperature": 0.7,
                "max_tokens": 1000
            }
        }
        
        print(f"ğŸ”§ DEBUG: Making HTTP POST request to {llm_config.base_url}/text-generation/generation")
        
        try:
            async with session.post(
                f"{llm_config.base_url}/text-generation/generation",
                headers=headers,
                json=data,
                timeout=30
            ) as response:
                print(f"ğŸ”§ DEBUG: HTTP response status: {response.status}")
                result = await response.json()
                print(f"ğŸ”§ DEBUG: HTTP response received, parsing JSON")
                response_text = result["output"]["text"]
                print(f"ğŸ”§ DEBUG: Extracted response text, length: {len(response_text)}")
                return response_text
        except Exception as e:
            print(f"ğŸ”§ DEBUG: Error in _call_qwen_api: {e}")
            raise
    
    async def _call_deepseek_api(
        self, 
        prompt: str, 
        llm_config: LLMConfig, 
        session: aiohttp.ClientSession
    ) -> str:
        """è°ƒç”¨DeepSeek API"""
        print(f"ğŸ”§ DEBUG: _call_deepseek_api started")
        
        headers = {
            "Authorization": f"Bearer {llm_config.api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": llm_config.model_name,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é“¶è¡Œä¸šåŠ¡åŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 1000
        }
        
        print(f"ğŸ”§ DEBUG: Making HTTP POST request to {llm_config.base_url}/v1/chat/completions")
        
        try:
            async with session.post(
                f"{llm_config.base_url}/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            ) as response:
                print(f"ğŸ”§ DEBUG: HTTP response status: {response.status}")
                result = await response.json()
                print(f"ğŸ”§ DEBUG: HTTP response received, parsing JSON")
                response_text = result["choices"][0]["message"]["content"]
                print(f"ğŸ”§ DEBUG: Extracted response text, length: {len(response_text)}")
                return response_text
        except Exception as e:
            print(f"ğŸ”§ DEBUG: Error in _call_deepseek_api: {e}")
            raise
    
    def _parse_llm_response(
        self, 
        response: str, 
        bank_name: str, 
        bank_code: str, 
        bank_id: int,
        samples_per_bank: int = 7
    ) -> List[Dict[str, Any]]:
        """
        è§£æLLMå“åº”
        
        Args:
            response: LLMå“åº”æ–‡æœ¬
            bank_name: é“¶è¡Œåç§°
            bank_code: è”è¡Œå·
            bank_id: é“¶è¡ŒID
            samples_per_bank: æ¯ä¸ªé“¶è¡Œç”Ÿæˆçš„æ ·æœ¬æ•°é‡
            
        Returns:
            è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        """
        try:
            # æå–JSONéƒ¨åˆ†
            import re
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                result = json.loads(json_match.group())
                questions = result.get("questions", [])
                
                # æ„å»ºè®­ç»ƒæ ·æœ¬
                samples = []
                for i, question in enumerate(questions[:samples_per_bank]):  # é™åˆ¶æŒ‡å®šæ•°é‡
                    if question and len(question.strip()) > 0:
                        samples.append({
                            "dataset_id": self.dataset_id,
                            "source_record_id": bank_id,
                            "question": question.strip(),
                            "answer": f"{bank_name}çš„è”è¡Œå·æ˜¯{bank_code}",
                            "question_type": "natural",
                            "split_type": "train",
                            "bank_name": bank_name,
                            "bank_code": bank_code,
                            "generated_at": datetime.utcnow()
                        })
                
                return samples
            else:
                logger.warning(f"No JSON found in LLM response for {bank_name}")
                return self._generate_rule_based_samples(bank_name, bank_code, bank_id, samples_per_bank)
                
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM JSON response: {e}")
            return self._generate_rule_based_samples(bank_name, bank_code, bank_id, samples_per_bank)
            logger.error(f"Failed to parse LLM JSON response: {e}")
            return self._generate_rule_based_samples(bank_name, bank_code, bank_id)
    
    def _generate_rule_based_samples(
        self, 
        bank_name: str, 
        bank_code: str, 
        bank_id: int,
        samples_per_bank: int = 7
    ) -> List[Dict[str, Any]]:
        """
        åŸºäºè§„åˆ™ç”Ÿæˆæ ·æœ¬ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        
        Args:
            bank_name: é“¶è¡Œåç§°
            bank_code: è”è¡Œå·
            bank_id: é“¶è¡ŒID
            samples_per_bank: æ¯ä¸ªé“¶è¡Œç”Ÿæˆçš„æ ·æœ¬æ•°é‡
            
        Returns:
            è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        """
        samples = []
        
        # 1. å®Œæ•´åç§°
        samples.append({
            "dataset_id": self.dataset_id,
            "source_record_id": bank_id,
            "question": bank_name,
            "answer": f"{bank_name}çš„è”è¡Œå·æ˜¯{bank_code}",
            "question_type": "exact",
            "split_type": "train",
            "bank_name": bank_name,
            "bank_code": bank_code,
            "generated_at": datetime.utcnow()
        })
        
        # 2. ç®€ç§°
        short_name = bank_name.replace("è‚¡ä»½æœ‰é™å…¬å¸", "").replace("æœ‰é™å…¬å¸", "")
        if short_name != bank_name and len(samples) < samples_per_bank:
            samples.append({
                "dataset_id": self.dataset_id,
                "source_record_id": bank_id,
                "question": short_name,
                "answer": f"{bank_name}çš„è”è¡Œå·æ˜¯{bank_code}",
                "question_type": "fuzzy",
                "split_type": "train",
                "bank_name": bank_name,
                "bank_code": bank_code,
                "generated_at": datetime.utcnow()
            })
        
        # 3-N. å…¶ä»–å˜ä½“
        variations = [
            f"{bank_name}çš„è”è¡Œå·",
            f"{short_name}è”è¡Œå·",
            f"{bank_name}ä»£ç ",
            f"{short_name}çš„ä»£ç æ˜¯å¤šå°‘",
            f"æŸ¥è¯¢{bank_name}è”è¡Œå·",
            f"{bank_name}é“¶è¡Œä»£ç ",
            f"{short_name}çš„è”è¡Œå·æ˜¯ä»€ä¹ˆ"
        ]
        
        for variation in variations:
            if len(samples) >= samples_per_bank:
                break
            samples.append({
                "dataset_id": self.dataset_id,
                "source_record_id": bank_id,
                "question": variation,
                "answer": f"{bank_name}çš„è”è¡Œå·æ˜¯{bank_code}",
                "question_type": "natural",
                "split_type": "train",
                "bank_name": bank_name,
                "bank_code": bank_code,
                "generated_at": datetime.utcnow()
            })
        
        return samples[:samples_per_bank]  # ç¡®ä¿è¿”å›æŒ‡å®šæ•°é‡
    
    def _save_samples_batch(self, samples: List[Dict[str, Any]]):
        """
        ä¼˜åŒ–çš„æ‰¹é‡ä¿å­˜æ ·æœ¬åˆ°æ•°æ®åº“
        
        Args:
            samples: æ ·æœ¬åˆ—è¡¨
        """
        if not samples:
            return
        
        db = self.SessionLocal()
        try:
            # ä½¿ç”¨æ‰¹é‡æ’å…¥ä¼˜åŒ–æ€§èƒ½
            batch_size = 1000  # æ¯æ‰¹1000æ¡è®°å½•
            total_saved = 0
            
            for i in range(0, len(samples), batch_size):
                batch = samples[i:i + batch_size]
                
                # ä½¿ç”¨bulk_insert_mappingsè¿›è¡Œé«˜æ•ˆæ‰¹é‡æ’å…¥
                db.bulk_insert_mappings(QAPair, batch)
                total_saved += len(batch)
                
                # æ¯æ‰¹æäº¤ä¸€æ¬¡ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
                if i % (batch_size * 5) == 0:  # æ¯5000æ¡æäº¤ä¸€æ¬¡
                    db.commit()
                    logger.info(f"Batch saved: {total_saved}/{len(samples)} samples")
            
            # æœ€ç»ˆæäº¤
            db.commit()
            
            self.stats["generated_samples"] += len(samples)
            logger.info(f"Successfully saved {len(samples)} samples to database in batches")
            
        except Exception as e:
            db.rollback()
            logger.error(f"Failed to save samples batch: {e}")
            raise
        finally:
            db.close()
    
    async def process_bank_batch_async(
        self, 
        banks: List[Dict[str, Any]], 
        llm_config: LLMConfig
    ):
        """
        å¼‚æ­¥å¤„ç†é“¶è¡Œæ‰¹æ¬¡
        
        Args:
            banks: é“¶è¡Œåˆ—è¡¨
            llm_config: LLMé…ç½®
        """
        async with aiohttp.ClientSession() as session:
            # æ§åˆ¶è¯·æ±‚é¢‘ç‡
            semaphore = asyncio.Semaphore(4)  # æ¯ä¸ªLLMæœ€å¤š4ä¸ªå¹¶å‘è¯·æ±‚
            
            async def process_single_bank(bank):
                async with semaphore:
                    try:
                        samples = await self.generate_samples_async(
                            bank["bank_name"],
                            bank["bank_code"],
                            bank["id"],
                            llm_config,
                            session
                        )
                        
                        # æ·»åŠ åˆ°ç»“æœé˜Ÿåˆ—
                        self.result_queue.put(samples)
                        self.stats["processed_banks"] += 1
                        
                        # è¯·æ±‚é—´éš”ï¼ˆé¿å…è¶…è¿‡APIé™åˆ¶ï¼‰
                        await asyncio.sleep(60 / llm_config.max_requests_per_minute)
                        
                    except Exception as e:
                        self.error_queue.put({
                            "bank": bank,
                            "error": str(e),
                            "llm": llm_config.name
                        })
                        self.stats["failed_banks"] += 1
            
            # å¹¶å‘å¤„ç†æ‰€æœ‰é“¶è¡Œ
            tasks = [process_single_bank(bank) for bank in banks]
            await asyncio.gather(*tasks, return_exceptions=True)
    
    def run_parallel_generation_for_banks(
        self, 
        bank_ids: List[int], 
        samples_per_bank: int = 7,
        use_llm: bool = False
    ) -> List[Dict[str, Any]]:
        """
        ä¸ºæŒ‡å®šçš„é“¶è¡ŒIDåˆ—è¡¨è¿è¡Œå¹¶è¡Œç”Ÿæˆ
        
        Args:
            bank_ids: é“¶è¡ŒIDåˆ—è¡¨
            samples_per_bank: æ¯ä¸ªé“¶è¡Œç”Ÿæˆçš„æ ·æœ¬æ•°é‡
            use_llm: æ˜¯å¦ä½¿ç”¨LLMï¼ˆFalse=è§„åˆ™ç”Ÿæˆï¼ŒTrue=LLMç”Ÿæˆï¼‰
        
        Returns:
            ç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        """
        logger.info(f"Starting parallel generation for {len(bank_ids)} specific banks...")
        print(f"ğŸ”§ DEBUG: Starting parallel generation for {len(bank_ids)} specific banks...")
        print(f"ğŸ”§ DEBUG: Bank IDs: {bank_ids[:10]}...")  # Show first 10 IDs
        self.stats["start_time"] = time.time()
        
        # è·å–æŒ‡å®šçš„é“¶è¡Œæ•°æ®
        db = self.SessionLocal()
        try:
            # ç¡®ä¿è·å–æœ€æ–°çš„æ•°æ®
            db.execute("BEGIN IMMEDIATE;")  # å¼ºåˆ¶åˆ·æ–°äº‹åŠ¡
            db.rollback()
            
            banks = []
            for bank_id in bank_ids:
                print(f"ğŸ”§ DEBUG: Looking for bank ID {bank_id}")
                record = db.query(BankCode).filter(BankCode.id == bank_id).first()
                if record:
                    banks.append({
                        "id": record.id,
                        "bank_name": record.bank_name,
                        "bank_code": record.bank_code
                    })
                    print(f"ğŸ”§ DEBUG: Found bank {record.id}: {record.bank_name}")
                else:
                    print(f"ğŸ”§ DEBUG: Bank ID {bank_id} not found!")
                    # å°è¯•æŸ¥è¯¢æ‰€æœ‰é“¶è¡Œçœ‹çœ‹æ•°æ®åº“è¿æ¥æ˜¯å¦æ­£å¸¸
                    total_count = db.query(BankCode).count()
                    print(f"ğŸ”§ DEBUG: Total banks in database: {total_count}")
                    if bank_id <= 5:  # åªå¯¹å‰å‡ ä¸ªIDåšè¯¦ç»†æ£€æŸ¥
                        all_ids = [r.id for r in db.query(BankCode.id).limit(10).all()]
                        print(f"ğŸ”§ DEBUG: First 10 bank IDs in database: {all_ids}")
            
            self.stats["total_banks"] = len(banks)
            logger.info(f"Found {len(banks)} banks to process")
            print(f"ğŸ”§ DEBUG: Found {len(banks)} banks to process")
            
        except Exception as e:
            print(f"ğŸ”§ DEBUG: Error querying banks: {e}")
            logger.error(f"Error querying banks: {e}")
        finally:
            db.close()
        
        if not banks:
            logger.warning("No banks found for processing")
            print("ğŸ”§ DEBUG: No banks found for processing")
            return []
        
        # ä½¿ç”¨è§„åˆ™ç”Ÿæˆæˆ–LLMç”Ÿæˆ
        all_samples = []
        
        if use_llm:
            # ä½¿ç”¨LLMå¹¶è¡Œç”Ÿæˆ
            all_samples = self._run_llm_parallel_generation(banks, samples_per_bank)
        else:
            # ä½¿ç”¨è§„åˆ™ç”Ÿæˆï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
            all_samples = self._run_rule_based_generation(banks, samples_per_bank)
        
        # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
        self._save_samples_batch(all_samples)
        
        # è¾“å‡ºç»Ÿè®¡
        elapsed = time.time() - self.stats["start_time"]
        logger.info(f"Parallel generation completed in {elapsed:.2f} seconds")
        logger.info(f"Generated {len(all_samples)} samples for {len(banks)} banks")
        
        return all_samples
    
    def run_parallel_generation_with_data(
        self, 
        banks_data: List[Dict[str, Any]], 
        samples_per_bank: int = 7,
        use_llm: bool = False
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ç›´æ¥ä¼ é€’çš„é“¶è¡Œæ•°æ®è¿è¡Œå¹¶è¡Œç”Ÿæˆï¼ˆé¿å…æ•°æ®åº“ä¼šè¯é—®é¢˜ï¼‰
        
        Args:
            banks_data: é“¶è¡Œæ•°æ®åˆ—è¡¨ [{"id": 1, "bank_name": "...", "bank_code": "..."}, ...]
            samples_per_bank: æ¯ä¸ªé“¶è¡Œç”Ÿæˆçš„æ ·æœ¬æ•°é‡
            use_llm: æ˜¯å¦ä½¿ç”¨LLMï¼ˆFalse=è§„åˆ™ç”Ÿæˆï¼ŒTrue=LLMç”Ÿæˆï¼‰
        
        Returns:
            ç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        """
        logger.info(f"Starting parallel generation with direct data for {len(banks_data)} banks...")
        print(f"ğŸ”§ DEBUG: Starting parallel generation with direct data for {len(banks_data)} banks...")
        self.stats["start_time"] = time.time()
        self.stats["total_banks"] = len(banks_data)
        
        if not banks_data:
            logger.warning("No banks data provided for processing")
            print("ğŸ”§ DEBUG: No banks data provided for processing")
            return []
        
        # ä½¿ç”¨è§„åˆ™ç”Ÿæˆæˆ–LLMç”Ÿæˆ
        all_samples = []
        
        if use_llm:
            # ä½¿ç”¨LLMå¹¶è¡Œç”Ÿæˆ
            all_samples = self._run_llm_parallel_generation(banks_data, samples_per_bank)
        else:
            # ä½¿ç”¨è§„åˆ™ç”Ÿæˆï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
            all_samples = self._run_rule_based_generation(banks_data, samples_per_bank)
        
        # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
        self._save_samples_batch(all_samples)
        
        # è¾“å‡ºç»Ÿè®¡
        elapsed = time.time() - self.stats["start_time"]
        logger.info(f"Parallel generation completed in {elapsed:.2f} seconds")
        logger.info(f"Generated {len(all_samples)} samples for {len(banks_data)} banks")
        
        return all_samples
    
    def _run_rule_based_generation(
        self, 
        banks: List[Dict[str, Any]], 
        samples_per_bank: int
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨è§„åˆ™ç”Ÿæˆï¼ˆå¤šçº¿ç¨‹å¹¶è¡Œï¼‰
        
        Args:
            banks: é“¶è¡Œåˆ—è¡¨
            samples_per_bank: æ¯ä¸ªé“¶è¡Œç”Ÿæˆçš„æ ·æœ¬æ•°é‡
        
        Returns:
            ç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        """
        logger.info("Using rule-based parallel generation...")
        all_samples = []
        
        def generate_for_bank(bank):
            """ä¸ºå•ä¸ªé“¶è¡Œç”Ÿæˆæ ·æœ¬"""
            samples = self._generate_rule_based_samples(
                bank["bank_name"],
                bank["bank_code"],
                bank["id"],
                samples_per_bank
            )
            self.stats["processed_banks"] += 1
            return samples
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_bank = {
                executor.submit(generate_for_bank, bank): bank 
                for bank in banks
            }
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_bank):
                try:
                    samples = future.result()
                    all_samples.extend(samples)
                    
                    # è¿›åº¦æŠ¥å‘Š
                    progress = (self.stats["processed_banks"] / self.stats["total_banks"]) * 100
                    if self.stats["processed_banks"] % 10 == 0:  # æ¯10ä¸ªé“¶è¡ŒæŠ¥å‘Šä¸€æ¬¡
                        logger.info(f"Progress: {progress:.1f}% ({self.stats['processed_banks']}/{self.stats['total_banks']})")
                        
                except Exception as e:
                    bank = future_to_bank[future]
                    logger.error(f"Failed to generate samples for {bank['bank_name']}: {e}")
                    self.stats["failed_banks"] += 1
        
        return all_samples
    
    def _run_llm_parallel_generation(
        self, 
        banks: List[Dict[str, Any]], 
        samples_per_bank: int
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨LLMå¹¶è¡Œç”Ÿæˆ
        
        Args:
            banks: é“¶è¡Œåˆ—è¡¨
            samples_per_bank: æ¯ä¸ªé“¶è¡Œç”Ÿæˆçš„æ ·æœ¬æ•°é‡
        
        Returns:
            ç”Ÿæˆçš„è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        """
        logger.info("Using LLM parallel generation...")
        print(f"ğŸ”§ DEBUG: Starting LLM parallel generation for {len(banks)} banks")
        
        # å°†é“¶è¡Œåˆ†é…ç»™ä¸åŒçš„LLM
        banks_per_llm = len(banks) // len(self.llm_configs)
        llm_bank_assignments = []
        
        print(f"ğŸ”§ DEBUG: Available LLM configs: {len(self.llm_configs)}")
        for i, llm_config in enumerate(self.llm_configs):
            print(f"ğŸ”§ DEBUG: LLM {i}: {llm_config.name}")
            start_idx = i * banks_per_llm
            if i == len(self.llm_configs) - 1:  # æœ€åä¸€ä¸ªLLMå¤„ç†å‰©ä½™çš„
                end_idx = len(banks)
            else:
                end_idx = (i + 1) * banks_per_llm
            
            assigned_banks = banks[start_idx:end_idx]
            llm_bank_assignments.append((llm_config, assigned_banks))
            logger.info(f"{llm_config.name} assigned {len(assigned_banks)} banks")
            print(f"ğŸ”§ DEBUG: {llm_config.name} assigned {len(assigned_banks)} banks")
        
        # å¯åŠ¨æ•°æ®åº“å†™å…¥çº¿ç¨‹
        all_samples = []
        sample_queue = Queue()
        
        def collect_samples():
            """æ”¶é›†æ ·æœ¬çš„çº¿ç¨‹"""
            print("ğŸ”§ DEBUG: Sample collector thread started")
            while True:
                try:
                    samples = sample_queue.get(timeout=10)
                    if samples is None:  # ç»“æŸä¿¡å·
                        print("ğŸ”§ DEBUG: Sample collector received end signal")
                        break
                    all_samples.extend(samples)
                    print(f"ğŸ”§ DEBUG: Collected {len(samples)} samples, total: {len(all_samples)}")
                except Exception as e:
                    print(f"ğŸ”§ DEBUG: Sample collector timeout or error: {e}")
                    continue
        
        collector_thread = threading.Thread(target=collect_samples)
        collector_thread.daemon = True
        collector_thread.start()
        
        print(f"ğŸ”§ DEBUG: Starting ThreadPoolExecutor with {len(self.llm_configs)} workers")
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ä¸åŒLLM
        with ThreadPoolExecutor(max_workers=len(self.llm_configs)) as executor:
            futures = []
            
            for i, (llm_config, assigned_banks) in enumerate(llm_bank_assignments):
                print(f"ğŸ”§ DEBUG: Submitting task {i} for {llm_config.name} with {len(assigned_banks)} banks")
                future = executor.submit(
                    self._process_banks_with_llm,
                    assigned_banks,
                    llm_config,
                    samples_per_bank,
                    sample_queue
                )
                futures.append(future)
            
            print(f"ğŸ”§ DEBUG: Submitted {len(futures)} tasks, waiting for completion...")
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            completed_count = 0
            for future in as_completed(futures):
                try:
                    result = future.result()
                    completed_count += 1
                    print(f"ğŸ”§ DEBUG: Task {completed_count}/{len(futures)} completed successfully")
                except Exception as e:
                    completed_count += 1
                    logger.error(f"LLM processing failed: {e}")
                    print(f"ğŸ”§ DEBUG: Task {completed_count}/{len(futures)} failed: {e}")
        
        print("ğŸ”§ DEBUG: All tasks completed, ending collector thread")
        
        # ç»“æŸæ”¶é›†çº¿ç¨‹
        sample_queue.put(None)
        collector_thread.join()
        
        print(f"ğŸ”§ DEBUG: LLM parallel generation completed, returning {len(all_samples)} samples")
        return all_samples
    
    def _process_banks_with_llm(
        self,
        banks: List[Dict[str, Any]],
        llm_config: LLMConfig,
        samples_per_bank: int,
        sample_queue: Queue
    ):
        """
        ä½¿ç”¨æŒ‡å®šLLMå¤„ç†é“¶è¡Œåˆ—è¡¨
        
        Args:
            banks: é“¶è¡Œåˆ—è¡¨
            llm_config: LLMé…ç½®
            samples_per_bank: æ¯ä¸ªé“¶è¡Œç”Ÿæˆçš„æ ·æœ¬æ•°é‡
            sample_queue: æ ·æœ¬æ”¶é›†é˜Ÿåˆ—
        """
        print(f"ğŸ”§ DEBUG: _process_banks_with_llm started for {llm_config.name} with {len(banks)} banks")
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            print(f"ğŸ”§ DEBUG: Starting async processing for {llm_config.name}")
            loop.run_until_complete(
                self._process_banks_async(banks, llm_config, samples_per_bank, sample_queue)
            )
            print(f"ğŸ”§ DEBUG: Async processing completed for {llm_config.name}")
        except Exception as e:
            print(f"ğŸ”§ DEBUG: Error in _process_banks_with_llm for {llm_config.name}: {e}")
            logger.error(f"Error in _process_banks_with_llm for {llm_config.name}: {e}")
        finally:
            loop.close()
            print(f"ğŸ”§ DEBUG: Event loop closed for {llm_config.name}")
    
    async def _process_banks_async(
        self,
        banks: List[Dict[str, Any]],
        llm_config: LLMConfig,
        samples_per_bank: int,
        sample_queue: Queue
    ):
        """
        å¼‚æ­¥å¤„ç†é“¶è¡Œåˆ—è¡¨
        """
        print(f"ğŸ”§ DEBUG: _process_banks_async started for {llm_config.name} with {len(banks)} banks")
        
        async with aiohttp.ClientSession() as session:
            print(f"ğŸ”§ DEBUG: HTTP session created for {llm_config.name}")
            semaphore = asyncio.Semaphore(4)  # æ¯ä¸ªLLMæœ€å¤š4ä¸ªå¹¶å‘è¯·æ±‚
            
            async def process_single_bank(bank):
                async with semaphore:
                    try:
                        print(f"ğŸ”§ DEBUG: Processing bank {bank['bank_name']} with {llm_config.name}")
                        samples = await self.generate_samples_async(
                            bank["bank_name"],
                            bank["bank_code"],
                            bank["id"],
                            llm_config,
                            session,
                            samples_per_bank
                        )
                        
                        sample_queue.put(samples)
                        self.stats["processed_banks"] += 1
                        print(f"ğŸ”§ DEBUG: Successfully processed {bank['bank_name']}, generated {len(samples)} samples")
                        
                        # è¿›åº¦æŠ¥å‘Š
                        progress = (self.stats["processed_banks"] / self.stats["total_banks"]) * 100
                        if self.stats["processed_banks"] % 5 == 0:
                            logger.info(f"LLM Progress: {progress:.1f}% ({self.stats['processed_banks']}/{self.stats['total_banks']})")
                        
                        # è¯·æ±‚é—´éš” - ä¼˜åŒ–å¤§æ•°æ®é›†å¤„ç†
                        if len(banks) > 10000:  # å¤§æ•°æ®é›†ä½¿ç”¨æ›´çŸ­é—´éš”
                            await asyncio.sleep(0.1)  # 100msé—´éš”ï¼Œæ¯ç§’10ä¸ªè¯·æ±‚
                        else:
                            await asyncio.sleep(60 / llm_config.max_requests_per_minute)
                        
                    except Exception as e:
                        print(f"ğŸ”§ DEBUG: Error processing {bank['bank_name']} with {llm_config.name}: {e}")
                        logger.error(f"Failed to process {bank['bank_name']} with {llm_config.name}: {e}")
                        # ä½¿ç”¨è§„åˆ™ç”Ÿæˆä½œä¸ºå¤‡ç”¨
                        samples = self._generate_rule_based_samples(
                            bank["bank_name"],
                            bank["bank_code"],
                            bank["id"],
                            samples_per_bank
                        )
                        sample_queue.put(samples)
                        self.stats["failed_banks"] += 1
                        print(f"ğŸ”§ DEBUG: Used fallback rule generation for {bank['bank_name']}, generated {len(samples)} samples")
            
            print(f"ğŸ”§ DEBUG: Creating {len(banks)} tasks for {llm_config.name}")
            # å¹¶å‘å¤„ç†æ‰€æœ‰é“¶è¡Œ
            tasks = [process_single_bank(bank) for bank in banks]
            print(f"ğŸ”§ DEBUG: Starting asyncio.gather for {len(tasks)} tasks")
            await asyncio.gather(*tasks, return_exceptions=True)
            print(f"ğŸ”§ DEBUG: asyncio.gather completed for {llm_config.name}")
        """
        è¿è¡Œå¹¶è¡Œç”Ÿæˆ
        
        Args:
            limit: é™åˆ¶å¤„ç†çš„é“¶è¡Œæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        logger.info("Starting parallel training data generation...")
        self.stats["start_time"] = time.time()
        
        # è·å–æ‰€æœ‰é“¶è¡Œæ•°æ®
        db = self.SessionLocal()
        try:
            query = db.query(BankCode)
            if limit:
                query = query.limit(limit)
            
            banks = []
            for record in query.all():
                banks.append({
                    "id": record.id,
                    "bank_name": record.bank_name,
                    "bank_code": record.bank_code
                })
            
            self.stats["total_banks"] = len(banks)
            logger.info(f"Found {len(banks)} banks to process")
            
        finally:
            db.close()
        
        # å°†é“¶è¡Œåˆ†é…ç»™ä¸åŒçš„LLM
        banks_per_llm = len(banks) // len(self.llm_configs)
        llm_bank_assignments = []
        
        for i, llm_config in enumerate(self.llm_configs):
            start_idx = i * banks_per_llm
            if i == len(self.llm_configs) - 1:  # æœ€åä¸€ä¸ªLLMå¤„ç†å‰©ä½™çš„
                end_idx = len(banks)
            else:
                end_idx = (i + 1) * banks_per_llm
            
            assigned_banks = banks[start_idx:end_idx]
            llm_bank_assignments.append((llm_config, assigned_banks))
            logger.info(f"{llm_config.name} assigned {len(assigned_banks)} banks")
        
        # å¯åŠ¨æ•°æ®åº“å†™å…¥çº¿ç¨‹
        db_writer_thread = threading.Thread(target=self._database_writer_worker)
        db_writer_thread.daemon = True
        db_writer_thread.start()
        
        # å¯åŠ¨è¿›åº¦ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=self._progress_monitor)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†ä¸åŒLLM
        with ThreadPoolExecutor(max_workers=len(self.llm_configs)) as executor:
            futures = []
            
            for llm_config, assigned_banks in llm_bank_assignments:
                future = executor.submit(
                    self._run_async_batch,
                    assigned_banks,
                    llm_config
                )
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    logger.error(f"LLM processing failed: {e}")
        
        # ç­‰å¾…æ•°æ®åº“å†™å…¥å®Œæˆ
        self.result_queue.put(None)  # ç»“æŸä¿¡å·
        db_writer_thread.join()
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        self._print_final_stats()
    
    def _run_async_batch(self, banks: List[Dict[str, Any]], llm_config: LLMConfig):
        """åœ¨çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥æ‰¹å¤„ç†"""
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(self.process_bank_batch_async(banks, llm_config))
        finally:
            loop.close()
    
    def _database_writer_worker(self):
        """æ•°æ®åº“å†™å…¥å·¥ä½œçº¿ç¨‹"""
        batch = []
        
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–ç»“æœ
                samples = self.result_queue.get(timeout=10)
                
                if samples is None:  # ç»“æŸä¿¡å·
                    # ä¿å­˜å‰©ä½™çš„æ‰¹æ¬¡
                    if batch:
                        self._save_samples_batch(batch)
                    break
                
                batch.extend(samples)
                
                # æ‰¹é‡ä¿å­˜
                if len(batch) >= self.batch_size:
                    self._save_samples_batch(batch)
                    batch = []
                
            except Exception as e:
                logger.error(f"Database writer error: {e}")
                continue
    
    def _progress_monitor(self):
        """è¿›åº¦ç›‘æ§çº¿ç¨‹"""
        while True:
            time.sleep(30)  # æ¯30ç§’æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            
            if self.stats["total_banks"] == 0:
                continue
            
            progress = (self.stats["processed_banks"] / self.stats["total_banks"]) * 100
            elapsed = time.time() - self.stats["start_time"]
            
            if self.stats["processed_banks"] > 0:
                avg_time_per_bank = elapsed / self.stats["processed_banks"]
                remaining_banks = self.stats["total_banks"] - self.stats["processed_banks"]
                eta = remaining_banks * avg_time_per_bank
                
                logger.info(
                    f"Progress: {progress:.1f}% "
                    f"({self.stats['processed_banks']}/{self.stats['total_banks']} banks) "
                    f"Generated: {self.stats['generated_samples']} samples "
                    f"Failed: {self.stats['failed_banks']} "
                    f"ETA: {eta/60:.1f} minutes"
                )
                
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if self.progress_callback:
                    try:
                        self.progress_callback(self.stats.copy())
                    except Exception as e:
                        logger.error(f"Progress callback error: {e}")
            
            # å¦‚æœå®Œæˆäº†ï¼Œé€€å‡ºç›‘æ§
            if self.stats["processed_banks"] + self.stats["failed_banks"] >= self.stats["total_banks"]:
                break
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        elapsed = time.time() - self.stats["start_time"]
        
        logger.info("=" * 60)
        logger.info("PARALLEL TRAINING DATA GENERATION COMPLETED")
        logger.info("=" * 60)
        logger.info(f"Total banks: {self.stats['total_banks']:,}")
        logger.info(f"Processed banks: {self.stats['processed_banks']:,}")
        logger.info(f"Failed banks: {self.stats['failed_banks']:,}")
        logger.info(f"Generated samples: {self.stats['generated_samples']:,}")
        logger.info(f"Total time: {elapsed/60:.1f} minutes")
        logger.info(f"Average time per bank: {elapsed/self.stats['total_banks']:.2f} seconds")
        logger.info(f"Samples per second: {self.stats['generated_samples']/elapsed:.2f}")
        logger.info("=" * 60)


    def run_parallel_generation(self, limit: Optional[int] = None):
        """
        è¿è¡Œå¹¶è¡Œç”Ÿæˆï¼ˆåŸæœ‰æ–¹æ³•ï¼Œä¿æŒå…¼å®¹æ€§ï¼‰
        
        Args:
            limit: é™åˆ¶å¤„ç†çš„é“¶è¡Œæ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        """
        logger.info("Starting parallel training data generation...")
        self.stats["start_time"] = time.time()
        
        # è·å–æ‰€æœ‰é“¶è¡Œæ•°æ®
        db = self.SessionLocal()
        try:
            query = db.query(BankCode)
            if limit:
                query = query.limit(limit)
            
            banks = []
            for record in query.all():
                banks.append({
                    "id": record.id,
                    "bank_name": record.bank_name,
                    "bank_code": record.bank_code
                })
            
            self.stats["total_banks"] = len(banks)
            logger.info(f"Found {len(banks)} banks to process")
            
        finally:
            db.close()
        
        if not banks:
            logger.warning("No banks found for processing")
            return []
        
        # ä½¿ç”¨è§„åˆ™ç”Ÿæˆï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
        all_samples = self._run_rule_based_generation(banks, 7)
        
        # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
        self._save_samples_batch(all_samples)
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        self._print_final_stats()
        
        return all_samples


def create_training_dataset(dataset_name: str = "å¤§è§„æ¨¡é“¶è¡Œè®­ç»ƒæ•°æ®é›†") -> int:
    """
    åˆ›å»ºè®­ç»ƒæ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        
    Returns:
        æ•°æ®é›†ID
    """
    db = next(get_db())
    try:
        dataset = Dataset(
            filename=f"{dataset_name}.json",
            file_path=f"generated/{dataset_name}_{int(datetime.utcnow().timestamp())}.json",
            file_size=0,  # å°†åœ¨ç”Ÿæˆå®Œæˆåæ›´æ–°
            total_records=0,  # å°†åœ¨ç”Ÿæˆå®Œæˆåæ›´æ–°
            valid_records=0,  # å°†åœ¨ç”Ÿæˆå®Œæˆåæ›´æ–°
            invalid_records=0,
            status='uploaded',
            uploaded_by=None  # ç³»ç»Ÿç”Ÿæˆ
        )
        db.add(dataset)
        db.commit()
        db.refresh(dataset)
        
        logger.info(f"Created dataset: {dataset_name} (ID: {dataset.id})")
        return dataset.id
        
    finally:
        db.close()


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆ›å»ºæ•°æ®é›†
    dataset_id = create_training_dataset()
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = ParallelTrainingGenerator(dataset_id)
    
    # è¿è¡Œç”Ÿæˆï¼ˆæµ‹è¯•æ—¶å¯ä»¥è®¾ç½®limitï¼‰
    generator.run_parallel_generation(limit=1000)  # æµ‹è¯•1000æ¡
    # generator.run_parallel_generation()  # ç”Ÿäº§ç¯å¢ƒå¤„ç†å…¨éƒ¨æ•°æ®