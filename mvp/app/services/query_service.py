"""
Query Service - æŸ¥è¯¢æ¨ç†æœåŠ¡

æœ¬æœåŠ¡è´Ÿè´£ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå®æ—¶æ¨ç†ï¼Œå¤„ç†ç”¨æˆ·çš„é“¶è¡Œè”è¡Œå·æŸ¥è¯¢è¯·æ±‚ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
    - æ¨¡å‹åŠ è½½ï¼šå¯åŠ¨æ—¶åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆLoRAæƒé‡ï¼‰
    - ç­”æ¡ˆç”Ÿæˆï¼šä½¿ç”¨æ¨¡å‹ä¸ºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆç­”æ¡ˆ
    - ä¿¡æ¯æå–ï¼šä»ç”Ÿæˆçš„ç­”æ¡ˆä¸­æå–é“¶è¡Œè”è¡Œå·ä¿¡æ¯
    - ç½®ä¿¡åº¦è®¡ç®—ï¼šè¯„ä¼°ç­”æ¡ˆçš„å¯ä¿¡åº¦
    - æŸ¥è¯¢æ—¥å¿—ï¼šè®°å½•æ‰€æœ‰æŸ¥è¯¢åˆ°æ•°æ®åº“
    - æ‰¹é‡æŸ¥è¯¢ï¼šæ”¯æŒæ‰¹é‡å¤„ç†å¤šä¸ªæŸ¥è¯¢
    - å†å²è®°å½•ï¼šæŸ¥è¯¢å†å²è®°å½•ç®¡ç†

æŠ€æœ¯æ ˆï¼š
    - transformers: HuggingFaceæ¨¡å‹æ¨ç†
    - peft: LoRAæƒé‡åŠ è½½
    - torch: PyTorchæ¨ç†å¼•æ“
    - SQLAlchemy: æ•°æ®åº“æ“ä½œ

ä½¿ç”¨ç¤ºä¾‹ï¼š
    >>> from app.services.query_service import QueryService
    >>> service = QueryService(db_session, model_path="models/job_1/final_model")
    >>> 
    >>> # å•ä¸ªæŸ¥è¯¢
    >>> response = service.query("å·¥å•†é“¶è¡Œçš„è”è¡Œå·æ˜¯å¤šå°‘ï¼Ÿ")
    >>> print(response["answer"])
    >>> 
    >>> # æ‰¹é‡æŸ¥è¯¢
    >>> questions = ["å·¥å•†é“¶è¡Œçš„è”è¡Œå·ï¼Ÿ", "å»ºè®¾é“¶è¡Œçš„è”è¡Œå·ï¼Ÿ"]
    >>> responses = service.batch_query(questions)
    >>> 
    >>> # æŸ¥è¯¢å†å²
    >>> history = service.get_query_history(user_id=1, limit=10)

æ¨ç†æµç¨‹ï¼š
    1. æ¥æ”¶ç”¨æˆ·æŸ¥è¯¢
    2. æ ¼å¼åŒ–ä¸ºæ¨¡å‹è¾“å…¥ï¼ˆæ·»åŠ æç¤ºè¯ï¼‰
    3. ä½¿ç”¨æ¨¡å‹ç”Ÿæˆç­”æ¡ˆ
    4. ä»ç­”æ¡ˆä¸­æå–é“¶è¡Œè”è¡Œå·
    5. åœ¨æ•°æ®åº“ä¸­æŸ¥æ‰¾å®Œæ•´ä¿¡æ¯
    6. è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°
    7. è®°å½•æŸ¥è¯¢æ—¥å¿—
    8. è¿”å›ç»“æ„åŒ–å“åº”
"""
import os
import re
import time
import hashlib
from functools import lru_cache
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    GenerationConfig
)
from peft import PeftModel
from sqlalchemy.orm import Session
from loguru import logger

from app.models.bank_code import BankCode
from app.models.training_job import TrainingJob
from app.models.query_log import QueryLog


class QueryServiceError(Exception):
    """
    æŸ¥è¯¢æœåŠ¡å¼‚å¸¸åŸºç±»
    
    ç”¨äºæ ‡è¯†æŸ¥è¯¢æœåŠ¡ä¸­çš„æ‰€æœ‰é”™è¯¯ï¼ŒåŒ…æ‹¬ï¼š
    - æ¨¡å‹åŠ è½½å¤±è´¥
    - ç­”æ¡ˆç”Ÿæˆå¤±è´¥
    - æŸ¥è¯¢å¤„ç†å¤±è´¥
    """
    pass


class QueryService:
    """
    æŸ¥è¯¢æ¨ç†æœåŠ¡ - ç”¨äºæ¨¡å‹æ¨ç†å’ŒæŸ¥è¯¢å¤„ç†
    
    æœ¬ç±»è´Ÿè´£åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶å¤„ç†ç”¨æˆ·çš„å®æ—¶æŸ¥è¯¢è¯·æ±‚ã€‚
    æ”¯æŒå•ä¸ªæŸ¥è¯¢å’Œæ‰¹é‡æŸ¥è¯¢ï¼Œè‡ªåŠ¨è®°å½•æŸ¥è¯¢æ—¥å¿—ã€‚
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
        1. æ¨¡å‹ç®¡ç†ï¼š
           - åŠ è½½åŸºç¡€æ¨¡å‹å’ŒLoRAæƒé‡
           - è‡ªåŠ¨æ£€æµ‹GPU/CPUè®¾å¤‡
           - è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        2. ç­”æ¡ˆç”Ÿæˆï¼š
           - æ ¼å¼åŒ–ç”¨æˆ·æŸ¥è¯¢ä¸ºæ¨¡å‹è¾“å…¥
           - ä½¿ç”¨é‡‡æ ·ç­–ç•¥ç”Ÿæˆç­”æ¡ˆ
           - ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–ç­”æ¡ˆéƒ¨åˆ†
        
        3. ä¿¡æ¯æå–ï¼š
           - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–12ä½è”è¡Œå·
           - åœ¨æ•°æ®åº“ä¸­æŸ¥æ‰¾å®Œæ•´çš„é“¶è¡Œä¿¡æ¯
           - æ”¯æŒæŒ‰bank_codeå’Œclearing_codeæŸ¥æ‰¾
        
        4. ç½®ä¿¡åº¦è¯„ä¼°ï¼š
           - åŸºäºå¯å‘å¼è§„åˆ™è®¡ç®—ç½®ä¿¡åº¦
           - è€ƒè™‘æ˜¯å¦æ‰¾åˆ°è”è¡Œå·
           - è€ƒè™‘ç­”æ¡ˆä¸­çš„å…³é”®è¯
        
        5. æŸ¥è¯¢æ—¥å¿—ï¼š
           - è®°å½•æ‰€æœ‰æŸ¥è¯¢åˆ°æ•°æ®åº“
           - åŒ…å«é—®é¢˜ã€ç­”æ¡ˆã€ç½®ä¿¡åº¦ã€å“åº”æ—¶é—´
           - æ”¯æŒæŒ‰ç”¨æˆ·IDè¿‡æ»¤
        
        6. å†å²ç®¡ç†ï¼š
           - æŸ¥è¯¢å†å²è®°å½•
           - æ”¯æŒåˆ†é¡µ
           - æ”¯æŒæŒ‰ç”¨æˆ·è¿‡æ»¤
    
    å±æ€§ï¼š
        db (Session): æ•°æ®åº“ä¼šè¯å¯¹è±¡
        base_model_name (str): åŸºç¡€æ¨¡å‹åç§°
        model_path (str): LoRAæƒé‡è·¯å¾„
        model: åŠ è½½çš„æ¨¡å‹å¯¹è±¡
        tokenizer: åˆ†è¯å™¨å¯¹è±¡
        device (str): æ¨ç†è®¾å¤‡ï¼ˆ"cuda"æˆ–"cpu"ï¼‰
        model_version (str): æ¨¡å‹ç‰ˆæœ¬æ ‡è¯†
    
    ä½¿ç”¨æµç¨‹ï¼š
        1. åˆ›å»ºQueryServiceå®ä¾‹ï¼ˆå¯é€‰æä¾›model_pathï¼‰
        2. å¦‚æœæœªåœ¨åˆå§‹åŒ–æ—¶åŠ è½½ï¼Œè°ƒç”¨load_model()
        3. è°ƒç”¨query()å¤„ç†å•ä¸ªæŸ¥è¯¢
        4. æˆ–è°ƒç”¨batch_query()å¤„ç†æ‰¹é‡æŸ¥è¯¢
        5. ä½¿ç”¨get_query_history()æŸ¥çœ‹å†å²è®°å½•
    """
    
    def __init__(
        self,
        db: Session,
        model_path: Optional[str] = None,
        base_model_name: str = "Qwen/Qwen2.5-0.5B"
    ):
        """
        Initialize Query Service
        
        Args:
            db: Database session
            model_path: Path to trained model weights (LoRA adapters)
            base_model_name: Base model name
        """
        self.db = db
        self.base_model_name = base_model_name
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        
        # æ£€æŸ¥è®¾å¤‡å¯ç”¨æ€§ï¼ˆä¼˜å…ˆä½¿ç”¨GPUï¼‰
        if torch.cuda.is_available():
            self.device = "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"
        
        self.model_version = None
        
        # æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜ç³»ç»Ÿ
        self.query_cache = {}
        self.cache_ttl = 3600  # 1å°æ—¶ç¼“å­˜
        self.max_cache_size = 1000
        self._cache_hits = 0
        self._total_queries = 0
        
        # åˆå§‹åŒ–RAGæœåŠ¡
        from app.services.rag_service import RAGService
        self.rag_service = RAGService(db)
        
        logger.info(f"QueryService initialized - Device: {self.device}")
        
        # Load model if path provided
        if model_path:
            self.load_model(model_path)
    
    def _get_cache_key(self, question: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        normalized = question.lower().strip()
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def _get_cached_result(self, question: str) -> Optional[Dict]:
        """è·å–ç¼“å­˜çš„æŸ¥è¯¢ç»“æœ"""
        cache_key = self._get_cache_key(question)
        
        if cache_key in self.query_cache:
            result, timestamp = self.query_cache[cache_key]
            
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ
            if time.time() - timestamp < self.cache_ttl:
                self._cache_hits += 1
                logger.info(f"Cache hit for question: {question[:30]}...")
                return result
            else:
                # åˆ é™¤è¿‡æœŸç¼“å­˜
                del self.query_cache[cache_key]
        
        return None
    
    def _check_memory_usage(self) -> Dict[str, Any]:
        """æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = {
            "device": self.device,
            "available": True
        }
        
        try:
            if self.device == "mps":
                # MPS å†…å­˜ä¿¡æ¯
                if torch.backends.mps.is_available():
                    # MPS æ²¡æœ‰ç›´æ¥çš„å†…å­˜æŸ¥è¯¢APIï¼Œä½†æˆ‘ä»¬å¯ä»¥è®°å½•çŠ¶æ€
                    memory_info["backend"] = "MPS"
                    memory_info["note"] = "MPS memory tracking limited"
            elif self.device == "cuda":
                # CUDA å†…å­˜ä¿¡æ¯
                memory_info["allocated_gb"] = torch.cuda.memory_allocated() / 1024**3
                memory_info["reserved_gb"] = torch.cuda.memory_reserved() / 1024**3
                memory_info["max_allocated_gb"] = torch.cuda.max_memory_allocated() / 1024**3
            
            logger.debug(f"Memory usage: {memory_info}")
        except Exception as e:
            logger.warning(f"Could not check memory usage: {e}")
        
        return memory_info

    
    def _cache_result(self, question: str, result: Dict):
        """ç¼“å­˜æŸ¥è¯¢ç»“æœ"""
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„æ¡ç›®
        if len(self.query_cache) >= self.max_cache_size:
            oldest_key = min(self.query_cache.keys(), 
                           key=lambda k: self.query_cache[k][1])
            del self.query_cache[oldest_key]
            logger.info("Cache full, removed oldest entry")
        
        cache_key = self._get_cache_key(question)
        self.query_cache[cache_key] = (result, time.time())
        logger.info(f"Cached result for question: {question[:30]}...")
    
    def get_cache_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        hit_rate = self._cache_hits / max(self._total_queries, 1)
        return {
            "cache_size": len(self.query_cache),
            "max_cache_size": self.max_cache_size,
            "cache_ttl": self.cache_ttl,
            "hit_rate": f"{hit_rate:.2%}",
            "total_queries": self._total_queries,
            "cache_hits": self._cache_hits
        }
    
    def load_model(self, model_path: str) -> None:
        """
        Load trained model and tokenizer
        
        Args:
            model_path: Path to trained model weights
        
        Raises:
            QueryServiceError: If model loading fails
        """
        try:
            logger.info(f"Loading model from: {model_path}")
            
            # æ¸…ç†å†…å­˜ï¼Œä¸ºæ–°æ¨¡å‹è…¾å‡ºç©ºé—´
            if self.model is not None:
                logger.info("Unloading previous model to free memory")
                del self.model
                del self.tokenizer
                self.model = None
                self.tokenizer = None
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶å’Œæ¸…ç†GPUç¼“å­˜
            import gc
            gc.collect()
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
                logger.info("MPS cache cleared")
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
                logger.info("CUDA cache cleared")
            
            # Determine the base model name from the training job
            # Extract job_id from model_path (e.g., "models/job_20/final_model" -> 20)
            base_model_name = self.base_model_name  # Default
            
            try:
                import re
                match = re.search(r'job_(\d+)', model_path)
                if match:
                    job_id = int(match.group(1))
                    # Query the training job to get the actual model name
                    from app.models.training_job import TrainingJob
                    job = self.db.query(TrainingJob).filter(TrainingJob.id == job_id).first()
                    if job and job.model_name:
                        base_model_name = job.model_name
                        logger.info(f"Using base model from training job: {base_model_name}")
            except Exception as e:
                logger.warning(f"Could not determine base model from training job, using default: {e}")
            
            # Load tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                base_model_name,
                trust_remote_code=True,
                padding_side="right"
            )
            
            # Set pad token if not exists
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # Load base model
            # æ ¹æ®è®¾å¤‡é€‰æ‹©åˆé€‚çš„æ•°æ®ç±»å‹å’Œé…ç½®
            if self.device == "cuda":
                torch_dtype = torch.float16
                device_map = "auto"
            elif self.device == "mps":
                torch_dtype = torch.float32  # MPSå¯¹float16æ”¯æŒæœ‰é™
                device_map = None
            else:
                torch_dtype = torch.float32
                device_map = None
            
            base_model = AutoModelForCausalLM.from_pretrained(
                base_model_name,
                trust_remote_code=True,
                torch_dtype=torch_dtype,
                device_map=device_map
            )
            
            # å¦‚æœæ˜¯MPSï¼Œæ‰‹åŠ¨ç§»åŠ¨æ¨¡å‹åˆ°MPSè®¾å¤‡
            if self.device == "mps":
                base_model = base_model.to(self.device)
            
            # Load LoRA adapters
            # MPSè®¾å¤‡éœ€è¦ç‰¹æ®Šå¤„ç†ï¼šå…ˆåŠ è½½åˆ°CPUï¼Œå†ç§»åŠ¨åˆ°MPS
            if self.device == "mps":
                # åœ¨CPUä¸ŠåŠ è½½LoRAæƒé‡
                self.model = PeftModel.from_pretrained(
                    base_model,
                    model_path,
                    torch_dtype=torch.float32
                )
                # æ‰‹åŠ¨ç§»åŠ¨åˆ°MPSè®¾å¤‡
                self.model = self.model.to("mps")
                logger.info("LoRA model loaded on MPS device")
            else:
                # CUDAæˆ–CPUç›´æ¥åŠ è½½
                lora_dtype = torch.float16 if self.device == "cuda" else torch.float32
                self.model = PeftModel.from_pretrained(
                    base_model,
                    model_path,
                    torch_dtype=lora_dtype
                )
            
            # Set to evaluation mode
            self.model.eval()
            
            # Extract model version from path
            self.model_version = Path(model_path).name
            
            logger.info(f"Model loaded successfully - Base: {base_model_name}, Version: {self.model_version}")
        
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            # æ¸…ç†å†…å­˜
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
            import gc
            gc.collect()
            raise QueryServiceError(f"Model loading failed: {e}")
    
    def generate_answer(
        self,
        question: str,
        max_new_tokens: int = 256,
        temperature: float = 0.1,  # é™ä½æ¸©åº¦å‡å°‘éšæœºæ€§å’Œå¹»è§‰
        top_p: float = 0.8,         # é™ä½top_pä½¿ç”Ÿæˆæ›´ä¿å®ˆ
        context: Optional[str] = None  # RAG context
    ) -> str:
        """
        Generate answer for a question using the model
        
        Args:
            question: User's question
            max_new_tokens: Maximum number of tokens to generate
            temperature: Sampling temperature
            top_p: Top-p sampling parameter
        
        Returns:
            Generated answer
        
        Raises:
            QueryServiceError: If generation fails
        """
        if self.model is None or self.tokenizer is None:
            raise QueryServiceError("Model not loaded. Call load_model() first.")
        
        try:
            # Format prompt - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ ¼å¼
            # If context provided (RAG mode), include it in the prompt
            if context:
                prompt = f"Reference Information:\n{context}\n\nQuestion: {question}\nAnswer:"
            else:
                prompt = f"Question: {question}\nAnswer:"
            
            # Log the prompt for debugging
            logger.info(f"RAG Prompt being sent to model: {prompt[:500]}...")
            
            # Tokenize
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=512
            )
            
            # Move to device (æ”¯æŒCUDAå’ŒMPS)
            if self.device in ["cuda", "mps"]:
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode
            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract answer (remove prompt) - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„åˆ†éš”ç¬¦
            if "Answer:" in full_text:
                answer = full_text.split("Answer:")[-1].strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°Answer:ï¼Œè¿”å›æ•´ä¸ªç”Ÿæˆçš„æ–‡æœ¬
                answer = full_text.strip()
            
            return answer
        
        except Exception as e:
            logger.error(f"Failed to generate answer: {e}")
            raise QueryServiceError(f"Answer generation failed: {e}")
    
    @lru_cache(maxsize=500)
    def format_structured_answer(
        self,
        question: str,
        matched_records: List[Dict[str, str]],
        confidence: float,
        response_time: float
    ) -> str:
        """
        æ ¼å¼åŒ–ç»“æ„åŒ–ç­”æ¡ˆè¾“å‡º - æ–°å¢åŠŸèƒ½
        
        ä¼˜åŒ–å†…å®¹ï¼š
        - æ ¹æ®æŸ¥è¯¢ç±»å‹æä¾›ä¸åŒçš„ç­”æ¡ˆæ ¼å¼
        - æ·»åŠ ç½®ä¿¡åº¦å’Œå“åº”æ—¶é—´ä¿¡æ¯
        - æä¾›ç”¨æˆ·å‹å¥½çš„é”™è¯¯æç¤ºå’Œå»ºè®®
        
        Args:
            question: åŸå§‹é—®é¢˜
            matched_records: åŒ¹é…çš„é“¶è¡Œè®°å½•
            confidence: ç½®ä¿¡åº¦åˆ†æ•°
            response_time: å“åº”æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
            
        Returns:
            ç»“æ„åŒ–çš„ç­”æ¡ˆå­—ç¬¦ä¸²
        """
        try:
            # å¦‚æœæ²¡æœ‰åŒ¹é…è®°å½•
            if not matched_records:
                return self._format_no_match_answer(question)
            
            # å•ä¸ªåŒ¹é…è®°å½•
            if len(matched_records) == 1:
                return self._format_single_match_answer(matched_records[0], confidence)
            
            # å¤šä¸ªåŒ¹é…è®°å½•
            return self._format_multiple_match_answer(matched_records, confidence)
            
        except Exception as e:
            logger.error(f"ç­”æ¡ˆæ ¼å¼åŒ–å¤±è´¥ï¼š{e}")
            return "æŠ±æ­‰ï¼Œç­”æ¡ˆæ ¼å¼åŒ–æ—¶å‡ºç°é”™è¯¯ã€‚"
    
    def _format_no_match_answer(self, question: str) -> str:
        """
        æ ¼å¼åŒ–æ— åŒ¹é…ç»“æœçš„ç­”æ¡ˆ
        
        Args:
            question: åŸå§‹é—®é¢˜
            
        Returns:
            æ— åŒ¹é…ç»“æœçš„å‹å¥½æç¤º
        """
        suggestions = []
        
        # åˆ†æé—®é¢˜å¹¶æä¾›å»ºè®®
        if len(question) < 3:
            suggestions.append("â€¢ è¯·æä¾›æ›´è¯¦ç»†çš„é“¶è¡Œåç§°æˆ–åœ°åŒºä¿¡æ¯")
        
        if not any(bank in question for bank in ["é“¶è¡Œ", "è¡Œ"]):
            suggestions.append("â€¢ è¯·ç¡®è®¤æŸ¥è¯¢çš„æ˜¯é“¶è¡Œæœºæ„")
        
        if not any(char.isdigit() for char in question):
            suggestions.append("â€¢ å¦‚æœæ‚¨çŸ¥é“éƒ¨åˆ†è”è¡Œå·ï¼Œå¯ä»¥åŒ…å«åœ¨æŸ¥è¯¢ä¸­")
        
        base_answer = "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°åŒ¹é…çš„é“¶è¡Œä¿¡æ¯ã€‚"
        
        if suggestions:
            base_answer += "\n\nå»ºè®®ï¼š\n" + "\n".join(suggestions)
        
        base_answer += "\n\næ‚¨ä¹Ÿå¯ä»¥å°è¯•ï¼š\nâ€¢ ä½¿ç”¨é“¶è¡Œå…¨ç§°ï¼ˆå¦‚ï¼šä¸­å›½å·¥å•†é“¶è¡Œè‚¡ä»½æœ‰é™å…¬å¸åŒ—äº¬è¥¿å•æ”¯è¡Œï¼‰\nâ€¢ åŒ…å«å…·ä½“åœ°åŒºä¿¡æ¯ï¼ˆå¦‚ï¼šåŒ—äº¬å·¥å•†é“¶è¡Œï¼‰"
        
        return base_answer
    
    def _format_single_match_answer(self, record: Dict[str, str], confidence: float) -> str:
        """
        æ ¼å¼åŒ–å•ä¸ªåŒ¹é…ç»“æœçš„ç­”æ¡ˆ
        
        Args:
            record: é“¶è¡Œè®°å½•
            confidence: ç½®ä¿¡åº¦
            
        Returns:
            æ ¼å¼åŒ–çš„å•ä¸ªåŒ¹é…ç­”æ¡ˆ
        """
        answer = f"ğŸ¦ {record['bank_name']}\nğŸ“‹ è”è¡Œå·ï¼š{record['bank_code']}"
        
        if record.get('clearing_code'):
            answer += f"\nğŸ”¢ æ¸…ç®—ä»£ç ï¼š{record['clearing_code']}"
        
        # æ ¹æ®ç½®ä¿¡åº¦æ·»åŠ ä¸åŒçš„æç¤º
        if confidence >= 0.9:
            answer += f"\nâœ… åŒ¹é…åº¦ï¼š{confidence:.1%}ï¼ˆé«˜åº¦åŒ¹é…ï¼‰"
        elif confidence >= 0.7:
            answer += f"\nâš ï¸ åŒ¹é…åº¦ï¼š{confidence:.1%}ï¼ˆè¾ƒå¥½åŒ¹é…ï¼‰"
        else:
            answer += f"\nâ“ åŒ¹é…åº¦ï¼š{confidence:.1%}ï¼ˆè¯·ç¡®è®¤æ˜¯å¦ä¸ºæ‚¨è¦æŸ¥æ‰¾çš„é“¶è¡Œï¼‰"
        
        return answer
    
    def _format_multiple_match_answer(self, records: List[Dict[str, str]], confidence: float) -> str:
        """
        æ ¼å¼åŒ–å¤šä¸ªåŒ¹é…ç»“æœçš„ç­”æ¡ˆ
        
        Args:
            records: é“¶è¡Œè®°å½•åˆ—è¡¨
            confidence: æ•´ä½“ç½®ä¿¡åº¦
            
        Returns:
            æ ¼å¼åŒ–çš„å¤šä¸ªåŒ¹é…ç­”æ¡ˆ
        """
        answer = f"æ‰¾åˆ° {len(records)} ä¸ªå¯èƒ½çš„åŒ¹é…ç»“æœï¼š\n\n"
        
        for i, record in enumerate(records[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ªç»“æœ
            answer += f"{i}. ğŸ¦ {record['bank_name']}\n"
            answer += f"   ğŸ“‹ è”è¡Œå·ï¼š{record['bank_code']}\n"
            if record.get('clearing_code'):
                answer += f"   ğŸ”¢ æ¸…ç®—ä»£ç ï¼š{record['clearing_code']}\n"
            answer += "\n"
        
        if len(records) > 5:
            answer += f"... è¿˜æœ‰ {len(records) - 5} ä¸ªç»“æœæœªæ˜¾ç¤º\n\n"
        
        answer += "ğŸ’¡ æç¤ºï¼šè¯·é€‰æ‹©æœ€ç¬¦åˆæ‚¨éœ€æ±‚çš„é“¶è¡Œï¼Œæˆ–æä¾›æ›´å…·ä½“çš„ä¿¡æ¯ä»¥è·å¾—ç²¾ç¡®åŒ¹é…ã€‚"
        
        return answer
        """
        ä½¿ç”¨å°æ¨¡å‹è¿›è¡Œé“¶è¡Œå®ä½“æå–
        
        æ¶æ„è¯´æ˜ï¼š
        1. LLMä»…ç”¨äºè®­ç»ƒæ•°æ®ç”Ÿæˆ
        2. å°æ¨¡å‹ç”¨äºè¯­ä¹‰å’Œè¯ä¹‰è§£æï¼ˆNERä»»åŠ¡ï¼‰
        3. å°æ¨¡å‹ç”¨äºæœ€ç»ˆç­”æ¡ˆæ±‡æ€»
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            DictåŒ…å«æå–çš„å®ä½“ï¼šbank_name, location, branch_nameç­‰
        """
        try:
            logger.info(f"Using small model for entity extraction: {question}")
            
            # TODO: æœªæ¥å¯ä»¥é›†æˆä¸“é—¨çš„NERå°æ¨¡å‹ï¼ˆå¦‚BERT-NERï¼‰
            # ç°é˜¶æ®µä½¿ç”¨è§„åˆ™æå–ï¼Œä½†æ¶æ„å·²ä¸ºå°æ¨¡å‹é¢„ç•™æ¥å£
            
            # ä½¿ç”¨è§„åˆ™æå–ï¼Œæ¨¡æ‹Ÿå°æ¨¡å‹NERçš„æ•ˆæœ
            fallback_keywords = []
            
            # é“¶è¡Œåç§°å®ä½“è¯†åˆ«
            bank_entities = {
                "åå¤é“¶è¡Œ": ["åå¤é“¶è¡Œ"],
                "ä¸­å›½å†œä¸šé“¶è¡Œ": ["ä¸­å›½å†œä¸šé“¶è¡Œ", "å†œä¸šé“¶è¡Œ", "å†œè¡Œ"],
                "ä¸­å›½å·¥å•†é“¶è¡Œ": ["ä¸­å›½å·¥å•†é“¶è¡Œ", "å·¥å•†é“¶è¡Œ", "å·¥è¡Œ"],
                "ä¸­å›½å»ºè®¾é“¶è¡Œ": ["ä¸­å›½å»ºè®¾é“¶è¡Œ", "å»ºè®¾é“¶è¡Œ", "å»ºè¡Œ"],
                "ä¸­å›½é“¶è¡Œ": ["ä¸­å›½é“¶è¡Œ", "ä¸­è¡Œ"],
                "äº¤é€šé“¶è¡Œ": ["äº¤é€šé“¶è¡Œ", "äº¤è¡Œ"],
                "æ‹›å•†é“¶è¡Œ": ["æ‹›å•†é“¶è¡Œ", "æ‹›è¡Œ"],
                "ä¸­ä¿¡é“¶è¡Œ": ["ä¸­ä¿¡é“¶è¡Œ"],
                "å…‰å¤§é“¶è¡Œ": ["å…‰å¤§é“¶è¡Œ"],
                "æ°‘ç”Ÿé“¶è¡Œ": ["æ°‘ç”Ÿé“¶è¡Œ"],
                "å…´ä¸šé“¶è¡Œ": ["å…´ä¸šé“¶è¡Œ"],
                "æµ¦å‘é“¶è¡Œ": ["æµ¦å‘é“¶è¡Œ", "ä¸Šæµ·æµ¦ä¸œå‘å±•é“¶è¡Œ"],
                "å¹³å®‰é“¶è¡Œ": ["å¹³å®‰é“¶è¡Œ"],
                "é‚®å‚¨é“¶è¡Œ": ["é‚®å‚¨é“¶è¡Œ", "é‚®æ”¿å‚¨è“„é“¶è¡Œ"],
                "å¹¿å‘é“¶è¡Œ": ["å¹¿å‘é“¶è¡Œ"],
                "æ¸¤æµ·é“¶è¡Œ": ["æ¸¤æµ·é“¶è¡Œ"],
                "æ’ä¸°é“¶è¡Œ": ["æ’ä¸°é“¶è¡Œ"],
                "æµ™å•†é“¶è¡Œ": ["æµ™å•†é“¶è¡Œ"]
            }
            
            found_bank = None
            for bank_name, aliases in bank_entities.items():
                for alias in aliases:
                    if alias in question:
                        fallback_keywords.append(bank_name)
                        if alias != bank_name:  # æ·»åŠ åˆ«åç”¨äºæœç´¢
                            fallback_keywords.append(alias)
                        found_bank = bank_name
                        break
                if found_bank:
                    break
            
            # åœ°ç†ä½ç½®å®ä½“è¯†åˆ«
            import re
            location_patterns = [
                # ç›´è¾–å¸‚
                r'(åŒ—äº¬|ä¸Šæµ·|å¤©æ´¥|é‡åº†)',
                # çœä¼šåŸå¸‚å’Œé‡è¦åŸå¸‚
                r'(å¦é—¨|æ·±åœ³|é’å²›|å¤§è¿|å®æ³¢|è‹å·|æ— é”¡|å¸¸å·|å—äº¬|æ­å·|æ¸©å·|å˜‰å…´|æ¹–å·|ç»å…´|é‡‘å|è¡¢å·|èˆŸå±±|å°å·|ä¸½æ°´)',
                r'(åˆè‚¥|èŠœæ¹–|èšŒåŸ |æ·®å—|é©¬éå±±|æ·®åŒ—|é“œé™µ|å®‰åº†|é»„å±±|æ»å·|é˜œé˜³|å®¿å·|å…­å®‰|äº³å·|æ± å·|å®£åŸ)',
                r'(ç¦å·|è†ç”°|ä¸‰æ˜|æ³‰å·|æ¼³å·|å—å¹³|é¾™å²©|å®å¾·)',
                r'(å—æ˜Œ|æ™¯å¾·é•‡|èä¹¡|ä¹æ±Ÿ|æ–°ä½™|é¹°æ½­|èµ£å·|å‰å®‰|å®œæ˜¥|æŠšå·|ä¸Šé¥¶)',
                r'(æµå—|æ·„åš|æ£åº„|ä¸œè¥|çƒŸå°|æ½åŠ|æµå®|æ³°å®‰|å¨æµ·|æ—¥ç…§|è±èŠœ|ä¸´æ²‚|å¾·å·|èŠåŸ|æ»¨å·|èæ³½)',
                r'(éƒ‘å·|å¼€å°|æ´›é˜³|å¹³é¡¶å±±|å®‰é˜³|é¹¤å£|æ–°ä¹¡|ç„¦ä½œ|æ¿®é˜³|è®¸æ˜Œ|æ¼¯æ²³|ä¸‰é—¨å³¡|å—é˜³|å•†ä¸˜|ä¿¡é˜³|å‘¨å£|é©»é©¬åº—)',
                r'(æ­¦æ±‰|é»„çŸ³|åå °|å®œæ˜Œ|è¥„é˜³|é„‚å·|è†é—¨|å­æ„Ÿ|è†å·|é»„å†ˆ|å’¸å®|éšå·|æ©æ–½)',
                r'(é•¿æ²™|æ ªæ´²|æ¹˜æ½­|è¡¡é˜³|é‚µé˜³|å²³é˜³|å¸¸å¾·|å¼ å®¶ç•Œ|ç›Šé˜³|éƒ´å·|æ°¸å·|æ€€åŒ–|å¨„åº•)',
                r'(å¹¿å·|éŸ¶å…³|ç æµ·|æ±•å¤´|ä½›å±±|æ±Ÿé—¨|æ¹›æ±Ÿ|èŒ‚å|è‚‡åº†|æƒ å·|æ¢…å·|æ±•å°¾|æ²³æº|é˜³æ±Ÿ|æ¸…è¿œ|ä¸œè|ä¸­å±±|æ½®å·|æ­é˜³|äº‘æµ®)',
                r'(å—å®|æŸ³å·|æ¡‚æ—|æ¢§å·|åŒ—æµ·|é˜²åŸæ¸¯|é’¦å·|è´µæ¸¯|ç‰æ—|ç™¾è‰²|è´ºå·|æ²³æ± |æ¥å®¾|å´‡å·¦)',
                r'(æµ·å£|ä¸‰äºš|ä¸‰æ²™|å„‹å·)',
                r'(æˆéƒ½|è‡ªè´¡|æ”€æèŠ±|æ³¸å·|å¾·é˜³|ç»µé˜³|å¹¿å…ƒ|é‚å®|å†…æ±Ÿ|ä¹å±±|å—å……|çœ‰å±±|å®œå®¾|å¹¿å®‰|è¾¾å·|é›…å®‰|å·´ä¸­|èµ„é˜³|æ±Ÿæ²¹)',
                r'(è´µé˜³|å…­ç›˜æ°´|éµä¹‰|å®‰é¡º|æ¯•èŠ‚|é“œä»)',
                r'(æ˜†æ˜|æ›²é–|ç‰æºª|ä¿å±±|æ˜­é€š|ä¸½æ±Ÿ|æ™®æ´±|ä¸´æ²§)',
                r'(æ‹‰è¨|æ˜Œéƒ½|å±±å—|æ—¥å–€åˆ™|é‚£æ›²|é˜¿é‡Œ|æ—èŠ)',
                r'(è¥¿å®‰|é“œå·|å®é¸¡|å’¸é˜³|æ¸­å—|å»¶å®‰|æ±‰ä¸­|æ¦†æ—|å®‰åº·|å•†æ´›)',
                r'(å…°å·|å˜‰å³ªå…³|é‡‘æ˜Œ|ç™½é“¶|å¤©æ°´|æ­¦å¨|å¼ æ–|å¹³å‡‰|é…’æ³‰|åº†é˜³|å®šè¥¿|é™‡å—)',
                r'(è¥¿å®|æµ·ä¸œ)',
                r'(é“¶å·|çŸ³å˜´å±±|å´å¿ |å›ºåŸ|ä¸­å«)',
                r'(ä¹Œé²æœ¨é½|å…‹æ‹‰ç›ä¾|åé²ç•ª|å“ˆå¯†)',
                # å¿çº§å¸‚å’Œç‰¹æ®Šåœ°å
                r'([^å¸‚å¿åŒºé•‡]{2,8}[å¸‚å¿åŒºé•‡])'
            ]
            
            found_location = None
            for pattern in location_patterns:
                location_match = re.search(pattern, question)
                if location_match:
                    found_location = location_match.group(1)
                    fallback_keywords.append(found_location)
                    break
            
            # æ”¯è¡Œåç§°å®ä½“è¯†åˆ«
            branch_patterns = [
                r'([^é“¶è¡Œ]{1,10}æ”¯è¡Œ)',
                r'([^é“¶è¡Œ]{1,10}åˆ†è¡Œ)',
            ]
            
            found_branch = None
            for pattern in branch_patterns:
                branch_match = re.search(pattern, question)
                if branch_match:
                    branch_name = branch_match.group(1)
                    if 'æ”¯è¡Œ' in branch_name:
                        found_branch = branch_name.replace('æ”¯è¡Œ', '')
                        fallback_keywords.append(found_branch)
                        fallback_keywords.append('æ”¯è¡Œ')
                    elif 'åˆ†è¡Œ' in branch_name:
                        found_branch = branch_name.replace('åˆ†è¡Œ', '')
                        fallback_keywords.append(found_branch)
                        fallback_keywords.append('åˆ†è¡Œ')
                    else:
                        found_branch = branch_name
                        fallback_keywords.append(found_branch)
                    break
            
            # æ„å»ºç»“æœ
            result = {
                "bank_name": found_bank or "æœªçŸ¥é“¶è¡Œ",
                "location": found_location or "æœªçŸ¥åœ°åŒº", 
                "branch_name": found_branch or "æœªçŸ¥æ”¯è¡Œ",
                "keywords": fallback_keywords if fallback_keywords else [question]
            }
            
            logger.info(f"Small model entity extraction result: {result}")
            return result
            
        except Exception as e:
            logger.error(f"Small model entity extraction failed: {e}")
            return {"keywords": [question]}

    def generate_answer_with_small_model(
        self,
        question: str,
        rag_results: List[Dict[str, str]],
        max_new_tokens: int = 128,
        temperature: float = 0.1
    ) -> str:
        """
        ä½¿ç”¨å°æ¨¡å‹åŸºäºRAGç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ - ä¼˜åŒ–ç‰ˆæœ¬
        
        ä¼˜åŒ–å†…å®¹ï¼š
        - æ”¹è¿›æ™ºèƒ½åŒ¹é…ç®—æ³•ï¼Œæå‡å¤šç»“æœåœºæ™¯ä¸‹çš„æœ€ä½³åŒ¹é…é€‰æ‹©
        - å¢å¼ºç›¸ä¼¼åº¦è®¡ç®—ï¼Œæ”¯æŒæ›´ç²¾ç¡®çš„é“¶è¡Œåç§°åŒ¹é…
        - ä¼˜åŒ–ç­”æ¡ˆæ ¼å¼åŒ–å’Œç»“æ„åŒ–è¾“å‡º
        - æ·»åŠ ç½®ä¿¡åº¦è¯„ä¼°å’Œè´¨é‡æ§åˆ¶
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            rag_results: RAGæ£€ç´¢ç»“æœ
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: ç”Ÿæˆæ¸©åº¦
            
        Returns:
            æ ¼å¼åŒ–çš„ç­”æ¡ˆ
        """
        try:
            if not rag_results:
                return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³é“¶è¡Œä¿¡æ¯ã€‚è¯·å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„é“¶è¡Œåç§°æˆ–åœ°åŒºä¿¡æ¯ã€‚"
            
            # å¦‚æœåªæœ‰ä¸€ä¸ªç»“æœï¼Œè¿›è¡Œè´¨é‡æ£€æŸ¥åè¿”å›
            if len(rag_results) == 1:
                bank = rag_results[0]
                confidence = self._calculate_single_result_confidence(question, bank)
                
                if confidence >= 0.7:
                    return self._format_single_answer(bank, confidence)
                else:
                    # ç½®ä¿¡åº¦è¾ƒä½æ—¶ï¼Œæä¾›æ›´å¤šä¿¡æ¯
                    return self._format_low_confidence_answer(bank, confidence)
            
            # å¤šä¸ªç»“æœæ—¶ï¼Œä½¿ç”¨ä¼˜åŒ–çš„æ™ºèƒ½åŒ¹é…ç®—æ³•
            logger.info(f"ä¼˜åŒ–ç­”æ¡ˆç”Ÿæˆï¼šä»{len(rag_results)}ä¸ªç»“æœä¸­é€‰æ‹©æœ€ä½³åŒ¹é…ï¼Œé—®é¢˜ï¼š{question}")
            
            # æå–é—®é¢˜ä¸­çš„å…³é”®ä¿¡æ¯ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰
            question_entities = self._extract_enhanced_entities(question)
            logger.info(f"å¢å¼ºå®ä½“æå–ç»“æœï¼š{question_entities}")
            
            # è®¡ç®—æ¯ä¸ªç»“æœçš„ç»¼åˆåŒ¹é…åˆ†æ•°
            scored_results = []
            for bank in rag_results:
                match_score = self._calculate_comprehensive_match_score(question, question_entities, bank)
                scored_results.append((bank, match_score))
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é€‰æ‹©æœ€ä½³åŒ¹é…
            scored_results.sort(key=lambda x: x[1]['total_score'], reverse=True)
            
            best_match, best_score_info = scored_results[0]
            logger.info(f"æœ€ä½³åŒ¹é…é€‰æ‹©ï¼š{best_match['bank_name']} "
                       f"(æ€»åˆ†ï¼š{best_score_info['total_score']:.2f}ï¼Œç½®ä¿¡åº¦ï¼š{best_score_info['confidence']:.2f})")
            
            # æ ¹æ®åŒ¹é…è´¨é‡å†³å®šè¿”å›ç­–ç•¥
            return self._generate_optimized_answer(question, scored_results, best_score_info)
            
        except Exception as e:
            logger.error(f"ä¼˜åŒ–ç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼š{e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚è¯·ç¨åé‡è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
    
    def _extract_enhanced_entities(self, question: str) -> Dict[str, Any]:
        """
        å¢å¼ºçš„å®ä½“æå–ï¼Œæ”¯æŒæ›´ç²¾ç¡®çš„é“¶è¡Œä¿¡æ¯è¯†åˆ«
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            å¢å¼ºçš„å®ä½“ä¿¡æ¯å­—å…¸
        """
        import re
        
        entities = {
            'bank_names': [],
            'locations': [],
            'branch_types': [],
            'keywords': [],
            'is_full_name': False,
            'query_type': 'general'
        }
        
        # æ£€æµ‹å®Œæ•´é“¶è¡Œåç§°
        if "è‚¡ä»½æœ‰é™å…¬å¸" in question and ("æ”¯è¡Œ" in question or "åˆ†è¡Œ" in question):
            entities['is_full_name'] = True
            entities['query_type'] = 'full_name'
            entities['keywords'].append(question.strip())
        
        # é“¶è¡Œåç§°è¯†åˆ«ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰
        bank_patterns = {
            'ä¸­å›½å·¥å•†é“¶è¡Œ': ['å·¥å•†é“¶è¡Œ', 'å·¥è¡Œ', 'ICBC', 'ä¸­å›½å·¥å•†'],
            'ä¸­å›½å†œä¸šé“¶è¡Œ': ['å†œä¸šé“¶è¡Œ', 'å†œè¡Œ', 'ABC', 'ä¸­å›½å†œä¸š'],
            'ä¸­å›½é“¶è¡Œ': ['ä¸­è¡Œ', 'BOC', 'ä¸­é“¶'],
            'ä¸­å›½å»ºè®¾é“¶è¡Œ': ['å»ºè®¾é“¶è¡Œ', 'å»ºè¡Œ', 'CCB', 'ä¸­å›½å»ºè®¾'],
            'äº¤é€šé“¶è¡Œ': ['äº¤è¡Œ', 'BOCOM', 'äº¤é€š'],
            'æ‹›å•†é“¶è¡Œ': ['æ‹›è¡Œ', 'CMB', 'æ‹›å•†'],
            'æµ¦å‘é“¶è¡Œ': ['ä¸Šæµ·æµ¦ä¸œå‘å±•é“¶è¡Œ', 'SPDB', 'æµ¦ä¸œå‘å±•'],
            'ä¸­ä¿¡é“¶è¡Œ': ['ä¸­ä¿¡', 'CITIC'],
            'å…‰å¤§é“¶è¡Œ': ['ä¸­å›½å…‰å¤§é“¶è¡Œ', 'CEB', 'å…‰å¤§'],
            'åå¤é“¶è¡Œ': ['åå¤', 'HXB'],
            'æ°‘ç”Ÿé“¶è¡Œ': ['ä¸­å›½æ°‘ç”Ÿé“¶è¡Œ', 'CMBC', 'æ°‘ç”Ÿ'],
            'å¹¿å‘é“¶è¡Œ': ['å¹¿å‘', 'CGB', 'å¹¿ä¸œå‘å±•é“¶è¡Œ'],
            'å¹³å®‰é“¶è¡Œ': ['å¹³å®‰', 'PAB'],
            'å…´ä¸šé“¶è¡Œ': ['å…´ä¸š', 'CIB'],
            'é‚®å‚¨é“¶è¡Œ': ['é‚®æ”¿å‚¨è“„é“¶è¡Œ', 'PSBC', 'é‚®å‚¨', 'é‚®æ”¿é“¶è¡Œ']
        }
        
        for full_name, aliases in bank_patterns.items():
            if full_name in question:
                entities['bank_names'].append(full_name)
                entities['keywords'].extend([full_name] + aliases)
                break
            else:
                for alias in aliases:
                    if alias in question:
                        entities['bank_names'].append(full_name)
                        entities['keywords'].extend([full_name, alias])
                        break
        
        # åœ°ç†ä½ç½®è¯†åˆ«ï¼ˆå¢å¼ºç‰ˆæœ¬ï¼‰
        location_patterns = [
            # ç›´è¾–å¸‚å’Œçœä¼š
            r'(åŒ—äº¬|ä¸Šæµ·|å¤©æ´¥|é‡åº†|å¹¿å·|æ·±åœ³|æˆéƒ½|æ­¦æ±‰|è¥¿å®‰|å—äº¬|æ­å·)',
            # é‡è¦åŸå¸‚
            r'(è‹å·|é’å²›|å¤§è¿|å®æ³¢|å¦é—¨|æ— é”¡|å¸¸å·|æ¸©å·|ä½›å±±|ä¸œè|ä¸­å±±)',
            # å•†ä¸šåŒºå’Œåœ°æ ‡
            r'(è¥¿å•|ç‹åºœäº•|ä¸­å…³æ‘|å›½è´¸|é‡‘èè¡—|é™†å®¶å˜´|å¤–æ»©|ç æ±Ÿæ–°åŸ|ç¦ç”°|å—å±±)'
        ]
        
        for pattern in location_patterns:
            matches = re.findall(pattern, question)
            entities['locations'].extend(matches)
            entities['keywords'].extend(matches)
        
        # æ”¯è¡Œç±»å‹è¯†åˆ«
        branch_patterns = [
            r'([^é“¶è¡Œ]{1,15}æ”¯è¡Œ)',
            r'([^é“¶è¡Œ]{1,15}åˆ†è¡Œ)',
            r'(è¥ä¸šéƒ¨|è¥ä¸šå…|åˆ†ç†å¤„|å‚¨è“„æ‰€)'
        ]
        
        for pattern in branch_patterns:
            matches = re.findall(pattern, question)
            entities['branch_types'].extend(matches)
            entities['keywords'].extend(matches)
        
        # æŸ¥è¯¢ç±»å‹åˆ¤æ–­
        if entities['is_full_name']:
            entities['query_type'] = 'full_name'
        elif entities['bank_names'] and entities['locations']:
            entities['query_type'] = 'bank_location'
        elif entities['bank_names']:
            entities['query_type'] = 'bank_only'
        elif entities['locations']:
            entities['query_type'] = 'location_only'
        
        return entities
    
    def _calculate_comprehensive_match_score(
        self, 
        question: str, 
        entities: Dict[str, Any], 
        bank: Dict[str, str]
    ) -> Dict[str, Any]:
        """
        è®¡ç®—ç»¼åˆåŒ¹é…åˆ†æ•°ï¼Œè€ƒè™‘å¤šä¸ªç»´åº¦
        
        Args:
            question: åŸå§‹é—®é¢˜
            entities: æå–çš„å®ä½“ä¿¡æ¯
            bank: é“¶è¡Œè®°å½•
            
        Returns:
            åŒ…å«è¯¦ç»†åˆ†æ•°ä¿¡æ¯çš„å­—å…¸
        """
        bank_name = bank['bank_name']
        bank_name_lower = bank_name.lower()
        question_lower = question.lower()
        
        score_info = {
            'exact_match_score': 0,
            'semantic_score': 0,
            'location_score': 0,
            'branch_score': 0,
            'penalty_score': 0,
            'rag_score': 0,
            'total_score': 0,
            'confidence': 0,
            'matched_features': []
        }
        
        # 1. ç²¾ç¡®åŒ¹é…åˆ†æ•°ï¼ˆæœ€é«˜æƒé‡ï¼‰
        if entities['is_full_name'] and question.strip() == bank_name:
            score_info['exact_match_score'] = 10000
            score_info['matched_features'].append('å®Œå…¨åŒ¹é…')
        else:
            # å…³é”®è¯ç²¾ç¡®åŒ¹é…
            for keyword in entities['keywords']:
                if len(keyword) >= 2 and keyword.lower() in bank_name_lower:
                    score_info['exact_match_score'] += len(keyword) * 100
                    score_info['matched_features'].append(f'å…³é”®è¯:{keyword}')
        
        # 2. è¯­ä¹‰åŒ¹é…åˆ†æ•°
        for bank_name_entity in entities['bank_names']:
            if bank_name_entity in bank_name:
                score_info['semantic_score'] += 1000
                score_info['matched_features'].append(f'é“¶è¡ŒåŒ¹é…:{bank_name_entity}')
        
        # 3. åœ°ç†ä½ç½®åŒ¹é…åˆ†æ•°
        for location in entities['locations']:
            if location in bank_name:
                score_info['location_score'] += 500
                score_info['matched_features'].append(f'åœ°ç†ä½ç½®:{location}')
        
        # 4. æ”¯è¡Œç±»å‹åŒ¹é…åˆ†æ•°
        for branch_type in entities['branch_types']:
            if branch_type in bank_name:
                score_info['branch_score'] += 300
                score_info['matched_features'].append(f'æ”¯è¡Œç±»å‹:{branch_type}')
        
        # 5. æƒ©ç½šåˆ†æ•°ï¼ˆé•¿åº¦å·®å¼‚è¿‡å¤§ã€æ— å…³åŒ¹é…ç­‰ï¼‰
        length_diff = abs(len(question) - len(bank_name))
        if length_diff > 30:
            score_info['penalty_score'] -= length_diff * 2
        
        # 6. RAGæ£€ç´¢åˆ†æ•°
        if 'final_score' in bank:
            score_info['rag_score'] = bank['final_score'] * 50
            score_info['matched_features'].append(f'RAGåˆ†æ•°:{bank["final_score"]:.3f}')
        
        # è®¡ç®—æ€»åˆ†
        score_info['total_score'] = (
            score_info['exact_match_score'] +
            score_info['semantic_score'] +
            score_info['location_score'] +
            score_info['branch_score'] +
            score_info['penalty_score'] +
            score_info['rag_score']
        )
        
        # è®¡ç®—ç½®ä¿¡åº¦
        score_info['confidence'] = min(1.0, max(0.0, score_info['total_score'] / 2000))
        
        return score_info
    
    def _calculate_single_result_confidence(self, question: str, bank: Dict[str, str]) -> float:
        """
        è®¡ç®—å•ä¸ªç»“æœçš„ç½®ä¿¡åº¦
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            bank: é“¶è¡Œè®°å½•
            
        Returns:
            ç½®ä¿¡åº¦åˆ†æ•° (0.0-1.0)
        """
        question_lower = question.lower()
        bank_name_lower = bank['bank_name'].lower()
        
        confidence = 0.0
        
        # å®Œå…¨åŒ¹é…
        if question.strip() == bank['bank_name']:
            confidence = 1.0
        # é«˜åº¦ç›¸ä¼¼
        elif question_lower in bank_name_lower or bank_name_lower in question_lower:
            confidence = 0.9
        # å…³é”®è¯åŒ¹é…
        else:
            common_chars = set(question_lower) & set(bank_name_lower)
            if len(common_chars) > 0:
                confidence = len(common_chars) / max(len(set(question_lower)), len(set(bank_name_lower)))
        
        # RAGåˆ†æ•°åŠ æˆ
        if 'final_score' in bank and bank['final_score'] > 0:
            confidence = min(1.0, confidence + bank['final_score'] * 0.1)
        
        return confidence
    
    def _format_single_answer(self, bank: Dict[str, str], confidence: float) -> str:
        """
        æ ¼å¼åŒ–å•ä¸ªç»“æœçš„ç­”æ¡ˆ
        
        Args:
            bank: é“¶è¡Œè®°å½•
            confidence: ç½®ä¿¡åº¦
            
        Returns:
            æ ¼å¼åŒ–çš„ç­”æ¡ˆ
        """
        if confidence >= 0.9:
            return f"{bank['bank_name']}: {bank['bank_code']}"
        else:
            return f"{bank['bank_name']}: {bank['bank_code']} (åŒ¹é…åº¦: {confidence:.1%})"
    
    def _format_low_confidence_answer(self, bank: Dict[str, str], confidence: float) -> str:
        """
        æ ¼å¼åŒ–ä½ç½®ä¿¡åº¦ç­”æ¡ˆ
        
        Args:
            bank: é“¶è¡Œè®°å½•
            confidence: ç½®ä¿¡åº¦
            
        Returns:
            æ ¼å¼åŒ–çš„ç­”æ¡ˆ
        """
        return (f"æ‰¾åˆ°å¯èƒ½åŒ¹é…çš„é“¶è¡Œï¼š{bank['bank_name']}: {bank['bank_code']}\n"
                f"åŒ¹é…åº¦è¾ƒä½ ({confidence:.1%})ï¼Œè¯·ç¡®è®¤æ˜¯å¦ä¸ºæ‚¨è¦æŸ¥æ‰¾çš„é“¶è¡Œã€‚")
    
    def _generate_optimized_answer(
        self, 
        question: str, 
        scored_results: List[Tuple[Dict[str, str], Dict[str, Any]]], 
        best_score_info: Dict[str, Any]
    ) -> str:
        """
        ç”Ÿæˆä¼˜åŒ–çš„ç­”æ¡ˆ
        
        Args:
            question: åŸå§‹é—®é¢˜
            scored_results: è¯„åˆ†ç»“æœåˆ—è¡¨
            best_score_info: æœ€ä½³åŒ¹é…çš„åˆ†æ•°ä¿¡æ¯
            
        Returns:
            ä¼˜åŒ–çš„ç­”æ¡ˆ
        """
        best_match = scored_results[0][0]
        
        # é«˜ç½®ä¿¡åº¦ï¼šç›´æ¥è¿”å›æœ€ä½³åŒ¹é…
        if best_score_info['confidence'] >= 0.8:
            return f"{best_match['bank_name']}: {best_match['bank_code']}"
        
        # ä¸­ç­‰ç½®ä¿¡åº¦ï¼šè¿”å›æœ€ä½³åŒ¹é…å¹¶æ ‡æ³¨ç½®ä¿¡åº¦
        elif best_score_info['confidence'] >= 0.5:
            return (f"{best_match['bank_name']}: {best_match['bank_code']}\n"
                   f"(åŒ¹é…åº¦: {best_score_info['confidence']:.1%})")
        
        # ä½ç½®ä¿¡åº¦ï¼šè¿”å›å¤šä¸ªå€™é€‰ç»“æœ
        else:
            top_results = scored_results[:min(3, len(scored_results))]
            answer_parts = ["æ‰¾åˆ°ä»¥ä¸‹å¯èƒ½çš„åŒ¹é…ç»“æœï¼š"]
            
            for i, (bank, score_info) in enumerate(top_results):
                confidence_str = f"åŒ¹é…åº¦: {score_info['confidence']:.1%}"
                answer_parts.append(f"{i+1}. {bank['bank_name']}: {bank['bank_code']} ({confidence_str})")
            
            answer_parts.append("è¯·é€‰æ‹©æœ€ç¬¦åˆæ‚¨éœ€æ±‚çš„é“¶è¡Œã€‚")
            return "\n".join(answer_parts)

    def retrieve_relevant_banks(self, question: str, top_k: int = 5) -> List[Dict[str, str]]:
        """
        Retrieve relevant bank records from database based on question
        
        This is the "Retrieval" part of RAG (Retrieval-Augmented Generation).
        
        Args:
            question: User's question
            top_k: Number of top results to return
        
        Returns:
            List of relevant bank records with name and code
        """
        try:
            logger.info(f"RAG: Starting retrieval for question: {question[:50]}...")
            
            # ä½¿ç”¨å°æ¨¡å‹æå–é“¶è¡Œå®ä½“
            entities = self.extract_bank_entities_with_small_model(question)
            keywords = entities.get("keywords", [])
            
            logger.info(f"RAG: LLM extracted entities: {entities}")
            logger.info(f"RAG: Using keywords for search: {keywords}")
            
            if not keywords:
                logger.warning("RAG: No keywords extracted")
                return []
            
            # æ„å»ºæ›´æ™ºèƒ½çš„æœç´¢ç­–ç•¥
            results = []
            seen_banks = set()
            
            # ä¼˜å…ˆæœç´¢ç»„åˆå…³é”®è¯
            if len(keywords) >= 2:
                for i in range(len(keywords)):
                    for j in range(i+1, len(keywords)):
                        combined_query = f"%{keywords[i]}%{keywords[j]}%"
                        logger.info(f"RAG: Searching with combined pattern: {combined_query}")
                        
                        records = self.db.query(BankCode).filter(
                            BankCode.bank_name.like(combined_query)
                        ).limit(top_k).all()
                        
                        logger.info(f"RAG: Found {len(records)} records for combined pattern")
                        for record in records:
                            if record.bank_name not in seen_banks:
                                results.append({
                                    "bank_name": record.bank_name,
                                    "bank_code": record.bank_code,
                                    "clearing_code": record.clearing_code
                                })
                                seen_banks.add(record.bank_name)
            
            # å¦‚æœç»„åˆæœç´¢ç»“æœä¸å¤Ÿï¼Œä½¿ç”¨å•ä¸ªå…³é”®è¯æœç´¢
            if len(results) < top_k:
                for keyword in keywords:
                    if len(keyword) >= 2:  # åªä½¿ç”¨2å­—ä»¥ä¸Šçš„å…³é”®è¯
                        logger.info(f"RAG: Searching for single keyword: {keyword}")
                        records = self.db.query(BankCode).filter(
                            BankCode.bank_name.contains(keyword)
                        ).limit(top_k).all()
                        
                        logger.info(f"RAG: Found {len(records)} records for keyword '{keyword}'")
                        for record in records:
                            if record.bank_name not in seen_banks and len(results) < top_k:
                                results.append({
                                    "bank_name": record.bank_name,
                                    "bank_code": record.bank_code,
                                    "clearing_code": record.clearing_code
                                })
                                seen_banks.add(record.bank_name)
            
            # è¿”å›ç»“æœ
            final_results = results[:top_k]
            logger.info(f"RAG: Returning {len(final_results)} results")
            for i, result in enumerate(final_results):
                logger.info(f"RAG: Result {i+1}: {result['bank_name']} -> {result['bank_code']}")
            
            return final_results
        
        except Exception as e:
            logger.error(f"Failed to retrieve relevant banks: {e}")
            return []
    
    def extract_bank_codes(self, answer: str) -> List[Dict[str, str]]:
        """
        Extract bank code information from answer
        
        Args:
            answer: Generated answer text
        
        Returns:
            List of extracted bank code records
        """
        extracted_records = []
        
        # Pattern to match 12-digit bank codes
        code_pattern = r'\b\d{12}\b'
        codes = re.findall(code_pattern, answer)
        
        # Look up codes in database
        for code in codes:
            # Try to find by bank_code
            record = self.db.query(BankCode).filter(
                BankCode.bank_code == code
            ).first()
            
            if record:
                extracted_records.append({
                    "bank_name": record.bank_name,
                    "bank_code": record.bank_code,
                    "clearing_code": record.clearing_code
                })
            else:
                # Try to find by clearing_code
                record = self.db.query(BankCode).filter(
                    BankCode.clearing_code == code
                ).first()
                
                if record:
                    extracted_records.append({
                        "bank_name": record.bank_name,
                        "bank_code": record.bank_code,
                        "clearing_code": record.clearing_code
                    })
        
        return extracted_records
    
    def calculate_confidence(
        self,
        answer: str,
        extracted_records: List[Dict[str, str]]
    ) -> float:
        """
        Calculate confidence score for the answer
        
        Args:
            answer: Generated answer
            extracted_records: Extracted bank code records
        
        Returns:
            Confidence score (0.0-1.0)
        """
        # Simple heuristic-based confidence calculation
        confidence = 0.0
        
        # If we found bank codes, increase confidence
        if extracted_records:
            confidence += 0.5
        
        # If answer contains specific keywords, increase confidence
        positive_keywords = ["è”è¡Œå·", "é“¶è¡Œ", "æ¸…ç®—", "åˆ†è¡Œ"]
        for keyword in positive_keywords:
            if keyword in answer:
                confidence += 0.1
        
        # If answer contains negative keywords, decrease confidence
        negative_keywords = ["ä¸ç¡®å®š", "å¯èƒ½", "ä¹Ÿè®¸", "æœªæ‰¾åˆ°", "ä¸çŸ¥é“"]
        for keyword in negative_keywords:
            if keyword in answer:
                confidence -= 0.2
        
        # Clamp to [0.0, 1.0]
        confidence = max(0.0, min(1.0, confidence))
        
        return confidence
    
    def query(
        self,
        question: str,
        user_id: Optional[int] = None,
        log_query: bool = True,
        use_rag: bool = True  # Enable RAG by default
    ) -> Dict[str, Any]:
        """
        Process a query and return response
        
        Args:
            question: User's question
            user_id: User ID (for logging)
            log_query: Whether to log the query
        
        Returns:
            Query response dictionary
        
        Raises:
            QueryServiceError: If query processing fails
        """
        start_time = time.time()
        self._total_queries += 1
        
        try:
            # æ€§èƒ½ä¼˜åŒ–ï¼šæ£€æŸ¥ç¼“å­˜
            cached_result = self._get_cached_result(question)
            if cached_result:
                # æ›´æ–°å“åº”æ—¶é—´ï¼ˆç¼“å­˜å‘½ä¸­å¾ˆå¿«ï¼‰
                cached_result['response_time'] = (time.time() - start_time) * 1000
                
                # è®°å½•æŸ¥è¯¢æ—¥å¿—ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if log_query and user_id:
                    self._log_query(
                        user_id=user_id,
                        question=question,
                        answer=cached_result['answer'],
                        confidence=cached_result['confidence'],
                        response_time=cached_result['response_time']
                    )
                
                return cached_result
            # RAG: Retrieve relevant banks using new vector-based RAG system
            context = None
            retrieved_banks = []
            logger.info(f"RAG enabled: {use_rag}")
            if use_rag:
                # ä½¿ç”¨æ–°çš„å‘é‡RAGç³»ç»Ÿ
                import asyncio
                try:
                    # åœ¨åŒæ­¥å‡½æ•°ä¸­è¿è¡Œå¼‚æ­¥RAGæ£€ç´¢
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    retrieved_banks = loop.run_until_complete(
                        self.rag_service.retrieve_relevant_banks(question, top_k=5)
                    )
                    loop.close()
                except Exception as rag_error:
                    logger.warning(f"Vector RAG failed, falling back to keyword search: {rag_error}")
                    # é™çº§åˆ°åŸæœ‰çš„å…³é”®è¯æ£€ç´¢
                    retrieved_banks = self.retrieve_relevant_banks(question, top_k=5)
                
                if retrieved_banks:
                    # Format context for the model
                    context_lines = []
                    for bank in retrieved_banks:
                        context_lines.append(
                            f"{bank['bank_name']}: {bank['bank_code']}"
                        )
                    context = "\n".join(context_lines)
                    logger.info(f"RAG: Retrieved {len(retrieved_banks)} relevant banks")
                    logger.info(f"RAG Context: {context[:200]}...")
                else:
                    logger.warning("RAG: No relevant banks found")
            else:
                logger.info("RAG: Disabled by request")
            
            # ä½¿ç”¨ä¼˜åŒ–çš„å°æ¨¡å‹ç­”æ¡ˆç”Ÿæˆç®—æ³•
            if retrieved_banks:
                logger.info("ä½¿ç”¨ä¼˜åŒ–çš„ç­”æ¡ˆç”Ÿæˆç®—æ³•...")
                answer = self.generate_answer_with_small_model(question, retrieved_banks)
            else:
                # ä½¿ç”¨ä¼˜åŒ–çš„æ— åŒ¹é…ç­”æ¡ˆæ ¼å¼åŒ–
                answer = self._format_no_match_answer(question)
            
            # è®°å½•ç”Ÿæˆçš„ç­”æ¡ˆï¼ˆè°ƒè¯•ç”¨ï¼‰
            logger.info(f"ä¼˜åŒ–ç­”æ¡ˆç”Ÿæˆå®Œæˆï¼ˆå‰200å­—ç¬¦ï¼‰ï¼š{answer[:200]}")
            
            # Extract bank codes
            matched_records = self.extract_bank_codes(answer)
            
            # è®°å½•æå–ç»“æœ
            logger.info(f"ä»ç­”æ¡ˆä¸­æå–äº†{len(matched_records)}æ¡é“¶è¡Œè®°å½•")
            
            if len(matched_records) == 0:
                logger.warning("æœªä»ç­”æ¡ˆä¸­æå–åˆ°é“¶è¡Œä»£ç ")
                # å¦‚æœæœ‰RAGç»“æœä½†æœªæå–åˆ°ä»£ç ï¼Œä½¿ç”¨RAGç»“æœ
                if retrieved_banks:
                    matched_records = retrieved_banks[:1]  # ä½¿ç”¨æœ€ä½³åŒ¹é…
                    logger.info("ä½¿ç”¨RAGæ£€ç´¢ç»“æœä½œä¸ºåŒ¹é…è®°å½•")
            
            # Calculate confidence with RAG enhancement
            confidence = self.calculate_confidence(answer, matched_records)
            
            # å¦‚æœæœ‰RAGç»“æœï¼Œè°ƒæ•´ç½®ä¿¡åº¦
            if retrieved_banks and 'final_score' in retrieved_banks[0]:
                rag_confidence = min(1.0, retrieved_banks[0]['final_score'] / 10.0)
                confidence = max(confidence, rag_confidence)
                logger.info(f"RAGç½®ä¿¡åº¦è°ƒæ•´ï¼š{rag_confidence:.3f}")
            
            # ä½¿ç”¨æ–°çš„ç»“æ„åŒ–ç­”æ¡ˆæ ¼å¼åŒ–ï¼ˆå¦‚æœæœ‰åŒ¹é…è®°å½•ï¼‰
            if matched_records:
                try:
                    formatted_answer = self.format_structured_answer(
                        question, matched_records, confidence, 0
                    )
                    # å¦‚æœæ ¼å¼åŒ–ç­”æ¡ˆæ›´å¥½ï¼Œä½¿ç”¨æ ¼å¼åŒ–ç‰ˆæœ¬
                    if len(formatted_answer) > len(answer) and "ğŸ¦" in formatted_answer:
                        answer = formatted_answer
                        logger.info("ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼åŒ–ç­”æ¡ˆ")
                except Exception as format_error:
                    logger.warning(f"ç­”æ¡ˆæ ¼å¼åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç­”æ¡ˆï¼š{format_error}")
            
            # Calculate response time
            response_time = (time.time() - start_time) * 1000  # Convert to milliseconds
            
            # Prepare response
            response = {
                "question": question,
                "answer": answer,
                "confidence": confidence,
                "response_time": response_time,
                "matched_records": matched_records,
                "timestamp": time.time()
            }
            
            # æ€§èƒ½ä¼˜åŒ–ï¼šç¼“å­˜ç»“æœï¼ˆåªç¼“å­˜æˆåŠŸçš„æŸ¥è¯¢ï¼‰
            if matched_records:  # åªç¼“å­˜æœ‰ç»“æœçš„æŸ¥è¯¢
                self._cache_result(question, response)
            
            # Log query to database
            if log_query:
                self._log_query(
                    user_id=user_id,
                    question=question,
                    answer=answer,
                    confidence=confidence,
                    response_time=response_time
                )
            
            return response
        
        except Exception as e:
            logger.error(f"Query processing failed: {e}")
            raise QueryServiceError(f"Query processing failed: {e}")
    
    def batch_query(
        self,
        questions: List[str],
        user_id: Optional[int] = None,
        log_queries: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Process multiple queries in batch
        
        Args:
            questions: List of questions
            user_id: User ID (for logging)
            log_queries: Whether to log the queries
        
        Returns:
            List of query responses
        """
        responses = []
        
        for question in questions:
            try:
                response = self.query(
                    question=question,
                    user_id=user_id,
                    log_query=log_queries
                )
                responses.append(response)
            except Exception as e:
                logger.error(f"Failed to process question '{question}': {e}")
                responses.append({
                    "question": question,
                    "answer": f"æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}",
                    "confidence": 0.0,
                    "response_time": 0.0,
                    "matched_records": [],
                    "error": str(e)
                })
        
        return responses
    
    def _log_query(
        self,
        user_id: Optional[int],
        question: str,
        answer: str,
        confidence: float,
        response_time: float
    ) -> None:
        """
        Log query to database
        
        Args:
            user_id: User ID
            question: Question
            answer: Answer
            confidence: Confidence score
            response_time: Response time in milliseconds
        """
        try:
            # ç¡®ä¿æ•°æ®åº“ä¼šè¯æ˜¯æ´»è·ƒçš„
            if not self.db.is_active:
                logger.warning("Database session is not active, attempting to refresh")
                self.db.rollback()  # é‡ç½®ä¼šè¯çŠ¶æ€
            
            query_log = QueryLog(
                user_id=user_id,
                question=question,
                answer=answer,
                confidence=confidence,
                response_time=response_time,
                model_version=self.model_version
            )
            self.db.add(query_log)
            self.db.commit()
            logger.info(f"Query logged successfully: user_id={user_id}, question='{question[:50]}...'")
        except Exception as e:
            logger.error(f"Failed to log query: {e}")
            try:
                self.db.rollback()
            except Exception as rollback_error:
                logger.error(f"Failed to rollback transaction: {rollback_error}")
    
    def get_query_history(
        self,
        user_id: Optional[int] = None,
        limit: int = 100,
        offset: int = 0
    ) -> List[Dict[str, Any]]:
        """
        Get query history from database
        
        Args:
            user_id: Filter by user ID (None for all users)
            limit: Maximum number of records to return
            offset: Number of records to skip
        
        Returns:
            List of query log records
        """
        try:
            query = self.db.query(QueryLog)
            
            if user_id is not None:
                query = query.filter(QueryLog.user_id == user_id)
            
            query = query.order_by(QueryLog.created_at.desc())
            query = query.limit(limit).offset(offset)
            
            logs = query.all()
            
            return [log.to_dict() for log in logs]
        
        except Exception as e:
            logger.error(f"Failed to get query history: {e}")
            return []
    
    def get_latest_model_path(self) -> Optional[str]:
        """
        Get the path to the latest trained model
        
        Returns:
            Path to latest model or None if no model found
        """
        try:
            # Find the latest completed training job
            latest_job = self.db.query(TrainingJob).filter(
                TrainingJob.status == "completed",
                TrainingJob.model_path.isnot(None)
            ).order_by(TrainingJob.completed_at.desc()).first()
            
            if latest_job and latest_job.model_path:
                model_path = latest_job.model_path
                if os.path.exists(model_path):
                    return model_path
            
            return None
        
        except Exception as e:
            logger.error(f"Failed to get latest model path: {e}")
            return None
