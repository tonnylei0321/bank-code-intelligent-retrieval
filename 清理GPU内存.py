#!/usr/bin/env python3
"""
GPUå†…å­˜æ¸…ç†å·¥å…·

ç”¨äºæ¸…ç†PyTorchåœ¨MPSè®¾å¤‡ä¸Šçš„å†…å­˜å ç”¨ï¼Œè§£å†³å†…å­˜ä¸è¶³é—®é¢˜ã€‚
"""

import torch
import gc
import os
import psutil

def get_memory_info():
    """è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯"""
    memory = psutil.virtual_memory()
    print(f"ç³»ç»Ÿå†…å­˜:")
    print(f"  æ€»å†…å­˜: {memory.total / (1024**3):.2f} GB")
    print(f"  å·²ä½¿ç”¨: {memory.used / (1024**3):.2f} GB")
    print(f"  å¯ç”¨å†…å­˜: {memory.available / (1024**3):.2f} GB")
    print(f"  ä½¿ç”¨ç‡: {memory.percent:.1f}%")

def clear_pytorch_cache():
    """æ¸…ç†PyTorchç¼“å­˜"""
    print("\næ¸…ç†PyTorchç¼“å­˜...")
    
    # æ¸…ç†CUDAç¼“å­˜ï¼ˆå¦‚æœæœ‰ï¼‰
    if torch.cuda.is_available():
        print("æ¸…ç†CUDAç¼“å­˜...")
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    # æ¸…ç†MPSç¼“å­˜ï¼ˆApple Siliconï¼‰
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        print("æ¸…ç†MPSç¼“å­˜...")
        torch.mps.empty_cache()
    
    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    print("æ‰§è¡Œåƒåœ¾å›æ”¶...")
    gc.collect()
    
    print("âœ… PyTorchç¼“å­˜æ¸…ç†å®Œæˆ")

def clear_model_cache_via_api():
    """é€šè¿‡APIæ¸…ç†æ¨¡å‹ç¼“å­˜"""
    try:
        import requests
        
        # è·å–ç®¡ç†å‘˜tokenï¼ˆéœ€è¦å…ˆç™»å½•ï¼‰
        print("\né€šè¿‡APIæ¸…ç†æ¨¡å‹ç¼“å­˜...")
        print("è¯·ç¡®ä¿å·²ç»ä»¥ç®¡ç†å‘˜èº«ä»½ç™»å½•ç³»ç»Ÿ")
        
        # è¿™é‡Œéœ€è¦å®é™…çš„tokenï¼Œç”¨æˆ·éœ€è¦æ‰‹åŠ¨æä¾›
        token = input("è¯·è¾“å…¥ç®¡ç†å‘˜tokenï¼ˆæˆ–æŒ‰Enterè·³è¿‡APIæ¸…ç†ï¼‰: ").strip()
        
        if token:
            headers = {"Authorization": f"Bearer {token}"}
            response = requests.post(
                "http://localhost:8001/api/v1/query/clear-model-cache",
                headers=headers
            )
            
            if response.status_code == 200:
                print("âœ… APIæ¨¡å‹ç¼“å­˜æ¸…ç†æˆåŠŸ")
                print(response.json().get('message', ''))
            else:
                print(f"âŒ APIæ¸…ç†å¤±è´¥: {response.status_code}")
                print(response.text)
        else:
            print("è·³è¿‡APIæ¸…ç†")
            
    except Exception as e:
        print(f"âŒ APIæ¸…ç†å‡ºé”™: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ GPUå†…å­˜æ¸…ç†å·¥å…·")
    print("=" * 50)
    
    # æ˜¾ç¤ºå†…å­˜ä¿¡æ¯
    get_memory_info()
    
    # æ¸…ç†PyTorchç¼“å­˜
    clear_pytorch_cache()
    
    # æ˜¾ç¤ºæ¸…ç†åçš„å†…å­˜ä¿¡æ¯
    print("\næ¸…ç†åçš„å†…å­˜çŠ¶æ€:")
    get_memory_info()
    
    # å°è¯•é€šè¿‡APIæ¸…ç†æ¨¡å‹ç¼“å­˜
    clear_model_cache_via_api()
    
    print("\nğŸ‰ å†…å­˜æ¸…ç†å®Œæˆï¼")
    print("\nğŸ’¡ å¦‚æœä»ç„¶æœ‰å†…å­˜é—®é¢˜ï¼Œå»ºè®®:")
    print("1. é‡å¯åç«¯æœåŠ¡")
    print("2. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚Qwen2.5-0.5Bï¼‰")
    print("3. è®¾ç½®ç¯å¢ƒå˜é‡: export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0")

if __name__ == "__main__":
    main()