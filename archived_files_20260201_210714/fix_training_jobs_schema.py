#!/usr/bin/env python3
"""
ä¿®å¤è®­ç»ƒä»»åŠ¡è¡¨ç»“æ„
"""
import sqlite3
import os
from datetime import datetime

def fix_training_jobs_schema():
    """ä¿®å¤è®­ç»ƒä»»åŠ¡è¡¨ç»“æ„"""
    db_path = "data/bank_code.db"
    
    if not os.path.exists(db_path):
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        print("ğŸ”§ ä¿®å¤è®­ç»ƒä»»åŠ¡è¡¨ç»“æ„...")
        
        # æ£€æŸ¥å½“å‰è¡¨ç»“æ„
        cursor.execute("PRAGMA table_info(training_jobs)")
        columns = cursor.fetchall()
        column_names = [col[1] for col in columns]
        
        print(f"å½“å‰å­—æ®µ: {column_names}")
        
        # æ£€æŸ¥ç¼ºå¤±çš„å­—æ®µ
        required_fields = {
            'retry_count': 'INTEGER DEFAULT 0',
            'max_retries': 'INTEGER DEFAULT 3', 
            'queued_at': 'DATETIME',
            'priority': 'VARCHAR(10) DEFAULT "medium"'
        }
        
        missing_fields = []
        for field, definition in required_fields.items():
            if field not in column_names:
                missing_fields.append((field, definition))
        
        if not missing_fields:
            print("âœ… è¡¨ç»“æ„å®Œæ•´ï¼Œæ— éœ€ä¿®å¤")
            return True
        
        print(f"ç¼ºå¤±å­—æ®µ: {[field for field, _ in missing_fields]}")
        
        # å¤‡ä»½åŸè¡¨
        backup_table = f"training_jobs_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        cursor.execute(f"CREATE TABLE {backup_table} AS SELECT * FROM training_jobs")
        print(f"âœ… å·²å¤‡ä»½åŸè¡¨ä¸º: {backup_table}")
        
        # é‡å»ºè¡¨ç»“æ„
        cursor.execute("DROP TABLE training_jobs")
        
        create_sql = """
        CREATE TABLE training_jobs (
            id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT,
            dataset_id INTEGER NOT NULL,
            created_by INTEGER NOT NULL,
            status VARCHAR(20) NOT NULL DEFAULT 'pending',
            model_name VARCHAR(100) NOT NULL DEFAULT 'Qwen/Qwen2.5-0.5B',
            epochs INTEGER NOT NULL DEFAULT 3,
            batch_size INTEGER NOT NULL DEFAULT 8,
            learning_rate FLOAT NOT NULL DEFAULT 0.0002,
            lora_r INTEGER NOT NULL DEFAULT 16,
            lora_alpha INTEGER NOT NULL DEFAULT 32,
            lora_dropout FLOAT NOT NULL DEFAULT 0.05,
            current_epoch INTEGER DEFAULT 0,
            total_steps INTEGER DEFAULT 0,
            current_step INTEGER DEFAULT 0,
            progress_percentage FLOAT DEFAULT 0.0,
            train_loss FLOAT,
            val_loss FLOAT,
            val_accuracy FLOAT,
            training_logs JSON DEFAULT '[]',
            model_path VARCHAR(500),
            error_message TEXT,
            retry_count INTEGER DEFAULT 0,
            max_retries INTEGER DEFAULT 3,
            queued_at DATETIME,
            priority VARCHAR(10) DEFAULT 'medium',
            created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            started_at DATETIME,
            completed_at DATETIME,
            updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(dataset_id) REFERENCES datasets (id) ON DELETE CASCADE,
            FOREIGN KEY(created_by) REFERENCES users (id)
        )
        """
        
        cursor.execute(create_sql)
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute("CREATE INDEX ix_training_jobs_id ON training_jobs (id)")
        
        # æ¢å¤æ•°æ®
        cursor.execute(f"""
            INSERT INTO training_jobs (
                id, dataset_id, created_by, status, model_name, epochs, batch_size,
                learning_rate, lora_r, lora_alpha, lora_dropout, current_epoch,
                total_steps, current_step, progress_percentage, train_loss, val_loss,
                val_accuracy, training_logs, model_path, error_message, created_at,
                started_at, completed_at, updated_at
            )
            SELECT 
                id, dataset_id, created_by, status, model_name, epochs, batch_size,
                learning_rate, lora_r, lora_alpha, lora_dropout, current_epoch,
                total_steps, current_step, progress_percentage, train_loss, val_loss,
                val_accuracy, training_logs, model_path, error_message, created_at,
                started_at, completed_at, updated_at
            FROM {backup_table}
        """)
        
        # éªŒè¯æ•°æ®æ¢å¤
        cursor.execute("SELECT COUNT(*) FROM training_jobs")
        new_count = cursor.fetchone()[0]
        
        cursor.execute(f"SELECT COUNT(*) FROM {backup_table}")
        backup_count = cursor.fetchone()[0]
        
        if new_count == backup_count:
            print(f"âœ… æ•°æ®æ¢å¤æˆåŠŸ: {new_count} æ¡è®°å½•")
            # åˆ é™¤å¤‡ä»½è¡¨
            cursor.execute(f"DROP TABLE {backup_table}")
            print("âœ… å·²åˆ é™¤å¤‡ä»½è¡¨")
        else:
            print(f"âš ï¸ æ•°æ®æ¢å¤å¼‚å¸¸: åŸ{backup_count}æ¡ï¼Œç°{new_count}æ¡")
        
        # éªŒè¯æ–°è¡¨ç»“æ„
        cursor.execute("PRAGMA table_info(training_jobs)")
        new_columns = cursor.fetchall()
        new_column_names = [col[1] for col in new_columns]
        
        print(f"æ–°è¡¨å­—æ®µ: {new_column_names}")
        
        # æ£€æŸ¥æ‰€æœ‰å¿…éœ€å­—æ®µæ˜¯å¦å­˜åœ¨
        all_present = all(field in new_column_names for field in required_fields.keys())
        
        if all_present:
            print("âœ… è¡¨ç»“æ„ä¿®å¤å®Œæˆ")
        else:
            print("âŒ è¡¨ç»“æ„ä¿®å¤å¤±è´¥")
            return False
        
        conn.commit()
        conn.close()
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        if 'conn' in locals():
            conn.rollback()
            conn.close()
        return False

def main():
    print("ğŸ”§ ä¿®å¤è®­ç»ƒä»»åŠ¡è¡¨ç»“æ„")
    print("=" * 50)
    
    success = fix_training_jobs_schema()
    
    if success:
        print("ğŸ‰ è®­ç»ƒä»»åŠ¡è¡¨ç»“æ„ä¿®å¤å®Œæˆï¼")
    else:
        print("âŒ è®­ç»ƒä»»åŠ¡è¡¨ç»“æ„ä¿®å¤å¤±è´¥")

if __name__ == "__main__":
    main()