#!/usr/bin/env python3
"""
æµ‹è¯•æ™ºèƒ½è®­ç»ƒå‚æ•°ä¼˜åŒ–åŠŸèƒ½
"""

import requests
import json
import time
from datetime import datetime

def test_intelligent_training():
    """æµ‹è¯•æ™ºèƒ½è®­ç»ƒå‚æ•°ä¼˜åŒ–"""
    
    base_url = "http://localhost:8000"
    
    # 1. ç™»å½•è·å–token
    print("1ï¸âƒ£ ç™»å½•è·å–è®¤è¯token...")
    login_response = requests.post(
        f"{base_url}/api/v1/auth/login",
        data={"username": "admin", "password": "admin123"},
        headers={"Content-Type": "application/x-www-form-urlencoded"}
    )
    
    if login_response.status_code != 200:
        print(f"âŒ ç™»å½•å¤±è´¥: {login_response.text}")
        return
    
    token = login_response.json()["access_token"]
    headers = {"Authorization": f"Bearer {token}"}
    print("âœ… ç™»å½•æˆåŠŸ")
    
    # 2. è·å–å¯ç”¨æ•°æ®é›†
    print("\n2ï¸âƒ£ è·å–å¯ç”¨æ•°æ®é›†...")
    datasets_response = requests.get(f"{base_url}/api/v1/datasets", headers=headers)
    
    if datasets_response.status_code != 200:
        print(f"âŒ è·å–æ•°æ®é›†å¤±è´¥: {datasets_response.text}")
        return
    
    datasets = datasets_response.json()
    if not datasets:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†")
        return
    
    # ä½¿ç”¨æœ€æ–°çš„æ•°æ®é›†
    dataset = datasets[0]
    dataset_id = dataset["id"]
    print(f"âœ… ä½¿ç”¨æ•°æ®é›†: ID={dataset_id}, æ–‡ä»¶={dataset['filename']}")
    
    # 3. æµ‹è¯•ä¸åŒæ¨¡å‹çš„å‚æ•°ä¼˜åŒ–
    models_to_test = [
        "Qwen/Qwen2.5-0.5B",
        "Qwen/Qwen2.5-1.5B"
    ]
    
    for model_name in models_to_test:
        print(f"\n3ï¸âƒ£ æµ‹è¯•æ¨¡å‹ {model_name} çš„å‚æ•°ä¼˜åŒ–...")
        
        # è·å–ä¼˜åŒ–å‚æ•°
        optimize_request = {
            "dataset_id": dataset_id,
            "model_name": model_name,
            "target_training_time_hours": 6.0  # ç›®æ ‡6å°æ—¶å®Œæˆ
        }
        
        optimize_response = requests.post(
            f"{base_url}/api/v1/training/optimize",
            json=optimize_request,
            headers=headers
        )
        
        if optimize_response.status_code != 200:
            print(f"âŒ å‚æ•°ä¼˜åŒ–å¤±è´¥: {optimize_response.text}")
            continue
        
        optimized_params = optimize_response.json()
        
        print(f"âœ… {model_name} ä¼˜åŒ–å‚æ•°:")
        print(f"   ğŸ“Š åŸºç¡€å‚æ•°:")
        print(f"      Epochs: {optimized_params['epochs']}")
        print(f"      Batch Size: {optimized_params['batch_size']}")
        print(f"      Learning Rate: {optimized_params['learning_rate']}")
        
        print(f"   ğŸ”§ LoRAå‚æ•°:")
        print(f"      LoRA R: {optimized_params['lora_r']}")
        print(f"      LoRA Alpha: {optimized_params['lora_alpha']}")
        print(f"      LoRA Dropout: {optimized_params['lora_dropout']}")
        
        print(f"   âš¡ ä¼˜åŒ–å‚æ•°:")
        print(f"      æ¢¯åº¦ç´¯ç§¯æ­¥æ•°: {optimized_params['gradient_accumulation_steps']}")
        print(f"      é¢„çƒ­æ­¥æ•°: {optimized_params['warmup_steps']}")
        print(f"      æƒé‡è¡°å‡: {optimized_params['weight_decay']}")
        
        print(f"   ğŸ“ˆ é¢„ä¼°ä¿¡æ¯:")
        print(f"      é¢„è®¡è®­ç»ƒæ—¶é—´: {optimized_params['estimated_training_time_hours']:.2f} å°æ—¶")
        print(f"      é¢„è®¡å†…å­˜ä½¿ç”¨: {optimized_params['estimated_memory_usage_gb']:.2f} GB")
        
        print(f"   ğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for note in optimized_params['optimization_notes']:
            print(f"      â€¢ {note}")
    
    # 4. æµ‹è¯•å¯åŠ¨æ™ºèƒ½ä¼˜åŒ–è®­ç»ƒ
    print(f"\n4ï¸âƒ£ å¯åŠ¨æ™ºèƒ½ä¼˜åŒ–è®­ç»ƒä»»åŠ¡...")
    
    training_request = {
        "dataset_id": dataset_id,
        "model_name": "Qwen/Qwen2.5-0.5B",  # ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹
        "use_optimized_params": True,
        "target_training_time_hours": 2.0  # ç›®æ ‡2å°æ—¶å®Œæˆ
    }
    
    training_response = requests.post(
        f"{base_url}/api/v1/training/start",
        json=training_request,
        headers=headers
    )
    
    if training_response.status_code != 201:
        print(f"âŒ å¯åŠ¨è®­ç»ƒå¤±è´¥: {training_response.text}")
        return
    
    job = training_response.json()
    job_id = job["id"]
    
    print(f"âœ… è®­ç»ƒä»»åŠ¡å·²å¯åŠ¨: ID={job_id}")
    print(f"   æ¨¡å‹: {job['model_name']}")
    print(f"   æ•°æ®é›†: {job['dataset_id']}")
    print(f"   é…ç½®: {job['epochs']} epochs, batch_size={job['batch_size']}")
    print(f"   å­¦ä¹ ç‡: {job['learning_rate']}")
    print(f"   LoRA: r={job['lora_r']}, alpha={job['lora_alpha']}")
    
    # 5. ç›‘æ§è®­ç»ƒè¿›åº¦
    print(f"\n5ï¸âƒ£ ç›‘æ§è®­ç»ƒè¿›åº¦...")
    
    for i in range(10):  # ç›‘æ§10æ¬¡
        time.sleep(30)  # ç­‰å¾…30ç§’
        
        status_response = requests.get(
            f"{base_url}/api/v1/training/{job_id}",
            headers=headers
        )
        
        if status_response.status_code != 200:
            print(f"âš ï¸ è·å–çŠ¶æ€å¤±è´¥: {status_response.text}")
            continue
        
        job_status = status_response.json()
        status = job_status["status"]
        progress = job_status.get("progress_percentage", 0)
        current_step = job_status.get("current_step", 0)
        total_steps = job_status.get("total_steps", 0)
        train_loss = job_status.get("train_loss")
        
        print(f"â° [{datetime.now().strftime('%H:%M:%S')}] çŠ¶æ€: {status}")
        if total_steps > 0:
            print(f"   è¿›åº¦: {current_step}/{total_steps} ({progress:.2f}%)")
        if train_loss:
            print(f"   æŸå¤±: {train_loss:.4f}")
        
        if status in ["completed", "failed"]:
            print(f"ğŸ è®­ç»ƒç»“æŸ: {status}")
            if status == "failed":
                error_msg = job_status.get("error_message", "æœªçŸ¥é”™è¯¯")
                print(f"âŒ é”™è¯¯ä¿¡æ¯: {error_msg}")
            break
    
    print("\nğŸ‰ æ™ºèƒ½è®­ç»ƒå‚æ•°ä¼˜åŒ–æµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    test_intelligent_training()