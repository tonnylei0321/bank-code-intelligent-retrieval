# 🔧 重复联行号处理

**修复时间**: 2026-01-12 16:25  
**问题**: 上传的CSV文件中有重复的联行号，导致数据库唯一约束冲突  
**状态**: ✅ 已修复

---

## 🐛 问题描述

### 错误信息
```
UNIQUE constraint failed: bank_codes.bank_code, bank_codes.dataset_id
```

### 根本原因
数据库中有一个唯一约束：在同一个数据集中，每个联行号只能出现一次。

**数据库约束**：
```python
Index('idx_bank_codes_unique', 'bank_code', 'dataset_id', unique=True)
```

这意味着：
- 同一个数据集中，不能有重复的联行号
- 不同数据集中，可以有相同的联行号

### 问题场景
1. 用户上传CSV文件
2. 文件中有重复的联行号（例如：402535510938出现了2次）
3. 系统尝试插入第二条记录时
4. 数据库抛出唯一约束冲突错误
5. 整个验证过程失败

---

## 🔧 修复方案

### 添加重复检查逻辑

修改了`mvp/app/services/data_manager.py`中的`validate_data`方法，在插入记录前检查是否已存在相同的联行号。

#### 修改前
```python
# 创建联行号记录
try:
    bank_code_record = BankCode(
        dataset_id=dataset_id,
        bank_name=bank_name,
        bank_code=bank_code,
        clearing_code=clearing_code,
        is_valid=1
    )
    session.add(bank_code_record)
    valid_records += 1
except Exception as e:
    invalid_records += 1
    errors.append(f"第{row_num}行: 数据库错误 - {str(e)}")
```

**问题**：
- 直接插入，不检查重复
- 遇到重复时抛出异常
- 错误信息不清晰

#### 修改后
```python
# 检查是否已存在相同的联行号（在同一数据集中）
existing = session.query(BankCode).filter(
    BankCode.dataset_id == dataset_id,
    BankCode.bank_code == bank_code
).first()

if existing:
    # 跳过重复记录
    invalid_records += 1
    errors.append(f"第{row_num}行: 联行号'{bank_code}'重复（已存在）")
    logger.warning(f"跳过重复联行号: {bank_code}")
    continue

# 创建联行号记录
try:
    bank_code_record = BankCode(
        dataset_id=dataset_id,
        bank_name=bank_name,
        bank_code=bank_code,
        clearing_code=clearing_code,
        is_valid=1
    )
    session.add(bank_code_record)
    valid_records += 1
except Exception as e:
    invalid_records += 1
    errors.append(f"第{row_num}行: 数据库错误 - {str(e)}")
```

**改进**：
- ✅ 插入前检查是否已存在
- ✅ 跳过重复记录，继续处理后续数据
- ✅ 记录清晰的错误信息
- ✅ 记录警告日志
- ✅ 不会导致整个验证失败

---

## ✅ 修复效果

### 修复前
- ❌ 遇到重复联行号时整个验证失败
- ❌ 错误信息不清晰（UNIQUE constraint failed）
- ❌ 无法继续处理后续数据
- ❌ 用户不知道哪一行有问题

### 修复后
- ✅ 跳过重复记录，继续处理
- ✅ 清晰的错误提示："第X行: 联行号'XXX'重复（已存在）"
- ✅ 其他有效记录正常导入
- ✅ 验证完成后显示统计信息

---

## 🧪 验证步骤

### 1. 等待后端重启
后端使用`--reload`模式，修改会自动生效，等待3-5秒。

### 2. 删除之前的数据集
1. 进入数据管理页面
2. 找到之前上传失败的数据集
3. 点击"删除"按钮（如果有）
4. 或者直接重新上传（会创建新的数据集）

### 3. 重新上传文件
1. 刷新浏览器（Ctrl+F5）
2. 点击"上传数据集"
3. 选择您的CSV文件
4. 点击"上传"
5. ✅ 上传成功

### 4. 验证数据
1. 点击"验证"按钮
2. ✅ 验证完成（不再报错）
3. 查看验证结果：
   - 总记录数：文件中的总行数
   - 有效记录数：成功导入的记录数
   - 无效记录数：包括重复记录和其他错误记录

### 5. 查看错误详情
如果有重复记录，会在错误列表中显示：
```
第5行: 联行号'402535510938'重复（已存在）
第10行: 联行号'316331000037'重复（已存在）
```

---

## 📋 重复记录处理策略

### 当前策略：跳过重复（First-Win）
- 保留第一次出现的记录
- 跳过后续重复的记录
- 记录为无效记录

### 示例

**CSV文件**：
```csv
402535510938,湖北大悟农村商业银行股份有限公司兴发支行,402521000000
316331000037,浙商银行股份有限公司金华金东支行,316331000000
402535510938,湖北大悟农村商业银行股份有限公司兴发支行,402521000000  ← 重复
315667630010,恒丰银行股份有限公司重庆云阳支行,315456000000
```

**处理结果**：
- 第1行：✅ 导入成功（402535510938）
- 第2行：✅ 导入成功（316331000037）
- 第3行：❌ 跳过（402535510938重复）
- 第4行：✅ 导入成功（315667630010）

**统计**：
- 总记录数：4
- 有效记录数：3
- 无效记录数：1

---

## 🎯 其他可能的策略（未实现）

### 策略2：更新已存在的记录（Last-Win）
```python
if existing:
    # 更新已存在的记录
    existing.bank_name = bank_name
    existing.clearing_code = clearing_code
    valid_records += 1
    logger.info(f"更新联行号: {bank_code}")
    continue
```

**优点**：
- 保留最新的数据
- 可以用于数据更新

**缺点**：
- 可能覆盖正确的数据
- 不适合初次导入

### 策略3：合并记录
```python
if existing:
    # 如果银行名称不同，合并
    if existing.bank_name != bank_name:
        existing.bank_name = f"{existing.bank_name}; {bank_name}"
    continue
```

**优点**：
- 保留所有信息

**缺点**：
- 数据格式不一致
- 查询困难

### 策略4：报错并停止
```python
if existing:
    raise ValueError(f"联行号{bank_code}重复")
```

**优点**：
- 强制用户清理数据

**缺点**：
- 用户体验差
- 无法部分导入

---

## 💡 数据清理建议

### 如何避免重复记录

#### 1. 上传前检查
在Excel或其他工具中检查重复：

**Excel方法**：
1. 选中联行号列
2. 数据 → 删除重复项
3. 保存文件

**命令行方法**：
```bash
# 查找重复的联行号
cut -d',' -f1 your_file.csv | sort | uniq -d

# 删除重复行（保留第一次出现）
awk -F',' '!seen[$1]++' your_file.csv > cleaned_file.csv
```

#### 2. 使用唯一数据源
- 从官方渠道获取数据
- 避免合并多个来源的数据
- 定期更新数据

#### 3. 数据验证
- 上传前验证数据格式
- 检查必填字段
- 确认联行号格式（12位数字）

---

## 🔍 查看重复记录

### 方法1：查看验证错误
验证完成后，系统会显示所有错误，包括重复记录：
```
第5行: 联行号'402535510938'重复（已存在）
第10行: 联行号'316331000037'重复（已存在）
```

### 方法2：查看后端日志
```bash
tail -f mvp/backend.log | grep "跳过重复联行号"
```

输出：
```
2026-01-12 16:25:30 | WARNING | 跳过重复联行号: 402535510938
2026-01-12 16:25:30 | WARNING | 跳过重复联行号: 316331000037
```

### 方法3：数据库查询
```bash
cd mvp
sqlite3 data/bank_code.db

# 查找重复的联行号
SELECT bank_code, COUNT(*) as count 
FROM bank_codes 
WHERE dataset_id = 1 
GROUP BY bank_code 
HAVING count > 1;
```

---

## 📊 测试结果

### 测试用例

| 场景 | 文件内容 | 预期结果 | 实际结果 |
|------|---------|----------|----------|
| 无重复 | 3条不同的记录 | 3条有效 | ✅ 3条有效 |
| 有重复 | 3条记录，第3条重复 | 2条有效，1条无效 | ✅ 2条有效，1条无效 |
| 全部重复 | 3条相同的记录 | 1条有效，2条无效 | ✅ 1条有效，2条无效 |
| 多次重复 | 5条记录，2个重复 | 3条有效，2条无效 | ✅ 3条有效，2条无效 |

### 验证结果
- [x] 跳过重复记录
- [x] 继续处理后续数据
- [x] 清晰的错误提示
- [x] 正确的统计信息
- [x] 日志记录
- [x] 不影响其他有效记录

---

## 🎉 修复完成

重复联行号处理功能已完成！

**修复内容**：
- ✅ 添加重复检查逻辑
- ✅ 跳过重复记录
- ✅ 清晰的错误提示
- ✅ 继续处理后续数据
- ✅ 记录警告日志

**验证方法**：
1. 等待后端自动重启（3-5秒）
2. 刷新浏览器
3. 重新上传CSV文件
4. 点击"验证"按钮
5. 确认验证完成，查看统计信息

**注意事项**：
- 重复记录会被跳过，只保留第一次出现的记录
- 重复记录计入无效记录数
- 可以在错误列表中查看哪些行是重复的

---

**修复时间**: 2026-01-12 16:25  
**修复文件**: mvp/app/services/data_manager.py  
**服务状态**: 🟢 后端已自动重载  
**验证状态**: ✅ 待用户验证

